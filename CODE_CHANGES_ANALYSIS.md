# ğŸ” CEMé¡¹ç›®ä»£ç æ”¹åŠ¨è¯¦ç»†åˆ†æ

## ğŸ“‹ é¡¹ç›®æ¦‚è§ˆ

| é¡¹ç›® | ä¸»è¦æ”¹åŠ¨ | æ–‡ä»¶æ•°é‡ | æ ¸å¿ƒæ€è·¯ |
|------|---------|---------|---------|
| CEM-att | ç”¨Attentionæ›¿æ¢GMM | ~8ä¸ªæ–‡ä»¶ | Slot + Cross Attention |
| CEM-mix | GMM + Attentionæ··åˆ | ~6ä¸ªæ–‡ä»¶ | å¹¶è¡Œæ··åˆæ¶æ„ |
| CEM-enhanced | ä¸²è¡Œå¢å¼ºæ¶æ„ | ~5ä¸ªæ–‡ä»¶ | ä¸²è¡Œ + å¤šå°ºåº¦ç‰¹å¾ |

---

## ğŸ¯ **CEM-att é¡¹ç›®æ”¹åŠ¨è¯¦æƒ…**

### ğŸ“ **æ–°å¢æ–‡ä»¶**
æ— æ–°å¢æ–‡ä»¶ï¼Œéƒ½æ˜¯ä¿®æ”¹åŸæœ‰æ–‡ä»¶

### ğŸ“ **ä¿®æ”¹çš„æ–‡ä»¶**

#### **1. model_training.py** (ä¸»è¦æ”¹åŠ¨)

**æ–°å¢ä»£ç å—1: Attentionæ¨¡å—å®šä¹‰ (ç¬¬41-137è¡Œ)**
```python
class SlotAttention(nn.Module):
    """
    Slot Attention module as requested by user:
    1. å…ˆå¯¹featureåšä¸€éslot attention
    2. ç„¶åæŠŠè¿™ä¸ªslot attentionä½œä¸ºä¸€ä¸ªKVè¾“åˆ°ä¸€ä¸ªcross attentioné‡Œé¢
    3. ç„¶åQå°±æ˜¯åŸfeature
    """
    def __init__(self, feature_dim, num_slots=8, num_iterations=3):
        super().__init__()
        self.num_slots = num_slots
        self.num_iterations = num_iterations
        self.feature_dim = feature_dim
        
        # Slot initialization parameters
        self.slot_mu = nn.Parameter(torch.randn(1, 1, feature_dim))
        self.slot_log_sigma = nn.Parameter(torch.zeros(1, 1, feature_dim))
        
        # Attention layers
        self.to_q = nn.Linear(feature_dim, feature_dim, bias=False)
        self.to_k = nn.Linear(feature_dim, feature_dim, bias=False) 
        self.to_v = nn.Linear(feature_dim, feature_dim, bias=False)
        
        # Slot update MLP
        self.slot_mlp = nn.Sequential(
            nn.LayerNorm(feature_dim),
            nn.Linear(feature_dim, feature_dim * 2),
            nn.ReLU(),
            nn.Linear(feature_dim * 2, feature_dim)
        )
        
    def forward(self, features):
        # features: [batch_size, feature_dim]
        batch_size = features.size(0)
        
        # Initialize slots
        mu = self.slot_mu.expand(batch_size, self.num_slots, -1)
        sigma = self.slot_log_sigma.exp().expand(batch_size, self.num_slots, -1)
        slots = mu + sigma * torch.randn_like(sigma)
        
        # Iterative attention updates
        for _ in range(self.num_iterations):
            # Attention mechanism
            q = self.to_q(slots)  # [batch_size, num_slots, feature_dim]
            k = self.to_k(features.unsqueeze(1))  # [batch_size, 1, feature_dim]
            v = self.to_v(features.unsqueeze(1))  # [batch_size, 1, feature_dim]
            
            # Compute attention weights
            attn = torch.softmax(torch.sum(q * k, dim=-1, keepdim=True) / (self.feature_dim ** 0.5), dim=1)
            
            # Update slots
            updates = torch.sum(attn * v, dim=1, keepdim=True)  # [batch_size, 1, feature_dim]
            slots = slots + self.slot_mlp(updates.expand(-1, self.num_slots, -1))
        
        return slots

class CrossAttention(nn.Module):
    """
    Cross Attention module where:
    - Q comes from original features
    - K,V come from slot attention outputs
    """
    def __init__(self, feature_dim, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.feature_dim = feature_dim
        self.head_dim = feature_dim // num_heads
        
        self.to_q = nn.Linear(feature_dim, feature_dim, bias=False)
        self.to_k = nn.Linear(feature_dim, feature_dim, bias=False)
        self.to_v = nn.Linear(feature_dim, feature_dim, bias=False)
        self.to_out = nn.Linear(feature_dim, feature_dim)
        
    def forward(self, query_features, slot_features):
        batch_size = query_features.size(0)
        
        # Generate Q, K, V
        q = self.to_q(query_features).view(batch_size, 1, self.num_heads, self.head_dim)
        k = self.to_k(slot_features).view(batch_size, -1, self.num_heads, self.head_dim)
        v = self.to_v(slot_features).view(batch_size, -1, self.num_heads, self.head_dim)
        
        # Attention computation
        attn = torch.softmax(torch.sum(q * k, dim=-1) / (self.head_dim ** 0.5), dim=1)
        out = torch.sum(attn.unsqueeze(-1) * v, dim=1)
        out = out.view(batch_size, self.feature_dim)
        
        return self.to_out(out)

class SlotCrossAttentionCEM(nn.Module):
    """
    Complete Slot + Cross Attention module for CEM conditional entropy calculation
    Replaces GMM in compute_class_means function
    """
    def __init__(self, feature_dim=128, num_slots=8):
        super().__init__()
        self.slot_attention = SlotAttention(feature_dim, num_slots)
        self.cross_attention = CrossAttention(feature_dim)
        
    def forward(self, features, labels, unique_labels):
        """
        Replace GMM-based conditional entropy calculation with attention mechanism
        """
        total_conditional_entropy = 0.0
        
        for label in unique_labels:
            # Get features for this class
            class_mask = (labels == label.item())
            if class_mask.sum() == 0:
                continue
                
            class_features = features[class_mask]
            
            if class_features.size(0) == 1:
                # Single sample, entropy is 0
                continue
                
            # Apply slot attention
            slot_representations = self.slot_attention(class_features)
            
            # Apply cross attention for each feature
            enhanced_features = []
            for i in range(class_features.size(0)):
                enhanced_feat = self.cross_attention(
                    class_features[i:i+1], 
                    slot_representations[i:i+1]
                )
                enhanced_features.append(enhanced_feat)
            
            enhanced_features = torch.cat(enhanced_features, dim=0)
            
            # Calculate conditional entropy based on enhanced features
            feature_variance = torch.var(enhanced_features, dim=0)
            conditional_entropy = torch.mean(torch.log(feature_variance + 1e-8))
            
            # Weight by class frequency
            class_weight = class_mask.sum().float() / labels.size(0)
            total_conditional_entropy += class_weight * conditional_entropy
        
        return total_conditional_entropy
```

**ä¿®æ”¹ä»£ç å—2: compute_class_meanså‡½æ•° (çº¦ç¬¬950-985è¡Œ)**
```python
def compute_class_means(self, features, labels, unique_labels, centroids_list):
    """
    ğŸš€ ATTENTION-BASED: ä½¿ç”¨Attentionæœºåˆ¶æ›¿ä»£GMMè®¡ç®—æ¡ä»¶ç†µ
    """
    # åˆå§‹åŒ–attentionæ¨¡å—
    if not hasattr(self, 'attention_cem'):
        # åŠ¨æ€ç¡®å®šfeature_dim
        if len(features.shape) == 4:  # [batch, channel, height, width]
            feature_dim = features.shape[1] * features.shape[2] * features.shape[3]
        else:  # [batch, feature_dim]
            feature_dim = features.shape[1]
            
        self.attention_cem = SlotCrossAttentionCEM(feature_dim=feature_dim).to(features.device)
    
    # å±•å¹³ç‰¹å¾
    if len(features.shape) == 4:
        features_flat = features.view(features.size(0), -1)
    else:
        features_flat = features
    
    # ä½¿ç”¨attentionè®¡ç®—æ¡ä»¶ç†µ
    rob_loss = self.attention_cem(features_flat, labels, unique_labels)
    intra_class_mse = torch.tensor(0.0)
    
    return rob_loss, intra_class_mse
```

#### **2. main_MIA.py** (ä¿®æ”¹)
**ä¿®æ”¹å†…å®¹**:
```python
# ç¬¬4-5è¡Œ: æ·»åŠ matplotlibåç«¯è®¾ç½®
import matplotlib
matplotlib.use('Agg')  # è®¾ç½®æ— æ˜¾ç¤ºåç«¯ï¼Œé¿å…Qté”™è¯¯

# ç¬¬8è¡Œ: ä¿®å¤å¯¼å…¥
import model_training,model_training_paral_pruning  # ç§»é™¤äº†model_training_paral
```

#### **3. main_test_MIA.py** (ä¿®æ”¹)
**ä¿®æ”¹å†…å®¹**:
```python
# ç¬¬4-5è¡Œ: æ·»åŠ matplotlibåç«¯è®¾ç½®
import matplotlib
matplotlib.use('Agg')  # è®¾ç½®æ— æ˜¾ç¤ºåç«¯ï¼Œé¿å…Qté”™è¯¯

# ç¬¬11è¡Œ: ä¿®å¤å¯¼å…¥
import model_training,model_training_paral_pruning  # ç§»é™¤äº†model_training_paral
```

#### **4. run_exp.sh** (ä¿®æ”¹)
**ä¿®æ”¹å†…å®¹**:
```bash
# ç¬¬27-28è¡Œ: ä¿®æ”¹å‚æ•°èŒƒå›´
regularization_strength_list="0.025"  # åŸæ¥æ˜¯ "0.01 0.025 0.05 0.1 0.15"
lambd_list="16"  # åŸæ¥æ˜¯ "0 8 16"
```

---

## ğŸ¯ **CEM-mix é¡¹ç›®æ”¹åŠ¨è¯¦æƒ…**

### ğŸ“ **ä¿®æ”¹çš„æ–‡ä»¶**

#### **1. model_training.py** (ä¸»è¦æ”¹åŠ¨)

**æ–°å¢ä»£ç å—1: Attentionæ¨¡å— (ç¬¬39-125è¡Œ)**
```python
class SlotAttention(nn.Module):
    """Slot Attention module for hybrid architecture"""
    def __init__(self, feature_dim, num_slots=8, num_iterations=3):
        super().__init__()
        self.num_slots = num_slots
        self.num_iterations = num_iterations
        self.feature_dim = feature_dim
        
        # Slot initialization parameters
        self.slot_mu = nn.Parameter(torch.randn(1, 1, feature_dim))
        self.slot_log_sigma = nn.Parameter(torch.zeros(1, 1, feature_dim))
        
        # Attention layers
        self.to_q = nn.Linear(feature_dim, feature_dim, bias=False)
        self.to_k = nn.Linear(feature_dim, feature_dim, bias=False) 
        self.to_v = nn.Linear(feature_dim, feature_dim, bias=False)
        
        # ç®€åŒ–çš„æ›´æ–°æœºåˆ¶ï¼Œé¿å…GRUé—®é¢˜
        self.slot_norm = nn.LayerNorm(feature_dim)
        self.slot_mlp = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 2),
            nn.ReLU(),
            nn.Linear(feature_dim * 2, feature_dim)
        )
        
    def forward(self, features):
        batch_size = features.size(0)
        
        # Initialize slots
        mu = self.slot_mu.expand(batch_size, self.num_slots, -1)
        sigma = self.slot_log_sigma.exp().expand(batch_size, self.num_slots, -1)
        slots = mu + sigma * torch.randn_like(sigma)
        
        # Iterative attention updates
        for _ in range(self.num_iterations):
            # Attention mechanism
            q = self.to_q(slots)
            k = self.to_k(features.unsqueeze(1))
            v = self.to_v(features.unsqueeze(1))
            
            # Compute attention weights
            attn = torch.softmax(torch.sum(q * k, dim=-1, keepdim=True) / (self.feature_dim ** 0.5), dim=1)
            
            # Update slots using LayerNorm + MLP instead of GRU
            updates = torch.sum(attn * v, dim=1, keepdim=True)
            slots = self.slot_norm(slots + self.slot_mlp(updates.expand(-1, self.num_slots, -1)))
        
        return slots

class CrossAttention(nn.Module):
    """Cross Attention module for hybrid architecture"""
    def __init__(self, feature_dim, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.feature_dim = feature_dim
        self.head_dim = feature_dim // num_heads
        
        self.to_q = nn.Linear(feature_dim, feature_dim, bias=False)
        self.to_k = nn.Linear(feature_dim, feature_dim, bias=False)
        self.to_v = nn.Linear(feature_dim, feature_dim, bias=False)
        self.to_out = nn.Linear(feature_dim, feature_dim)
        
    def forward(self, query_features, slot_features):
        batch_size = query_features.size(0)
        
        q = self.to_q(query_features).view(batch_size, 1, self.num_heads, self.head_dim)
        k = self.to_k(slot_features).view(batch_size, -1, self.num_heads, self.head_dim)
        v = self.to_v(slot_features).view(batch_size, -1, self.num_heads, self.head_dim)
        
        attn = torch.softmax(torch.sum(q * k, dim=-1) / (self.head_dim ** 0.5), dim=1)
        out = torch.sum(attn.unsqueeze(-1) * v, dim=1)
        out = out.view(batch_size, self.feature_dim)
        
        return self.to_out(out)
```

**æ–°å¢ä»£ç å—2: è‡ªé€‚åº”æƒé‡æ¨¡å— (ç¬¬127-168è¡Œ)**
```python
class AdaptiveWeightModule(nn.Module):
    """åŠ¨æ€è°ƒèŠ‚GMMå’ŒAttentionæƒé‡çš„æ¨¡å—"""
    def __init__(self, feature_dim):
        super().__init__()
        self.feature_dim = feature_dim
        self.weight_predictor = nn.Sequential(
            nn.Linear(feature_dim, feature_dim // 4),
            nn.ReLU(),
            nn.Linear(feature_dim // 4, feature_dim // 8),
            nn.ReLU(),
            nn.Linear(feature_dim // 8, 2),  # è¾“å‡ºGMMå’ŒAttentionçš„æƒé‡
            nn.Softmax(dim=-1)
        )
        
    def forward(self, features):
        """
        æ ¹æ®ç‰¹å¾å¤æ‚åº¦é¢„æµ‹GMMå’ŒAttentionçš„èåˆæƒé‡
        """
        # è®¡ç®—ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
        feature_mean = torch.mean(features, dim=0)
        feature_var = torch.var(features, dim=0)
        feature_std = torch.std(features, dim=0)
        
        # ç‰¹å¾å¤æ‚åº¦æŒ‡æ ‡
        complexity_features = torch.cat([
            feature_mean,
            feature_var, 
            feature_std,
            torch.mean(torch.abs(features), dim=0),  # å¹³å‡ç»å¯¹å€¼
            torch.max(features, dim=0)[0],  # æœ€å¤§å€¼
            torch.min(features, dim=0)[0]   # æœ€å°å€¼
        ])
        
        # é¢„æµ‹æƒé‡
        weights = self.weight_predictor(complexity_features)
        return weights[0], weights[1]  # gmm_weight, attention_weight
```

**æ–°å¢ä»£ç å—3: æ··åˆæ¶æ„æ ¸å¿ƒ (ç¬¬170-321è¡Œ)**
```python
class HybridGMMAttentionCEM(nn.Module):
    """æ··åˆGMMå’ŒAttentionçš„CEMæ¡ä»¶ç†µè®¡ç®—æ¨¡å—"""
    def __init__(self, feature_dim, num_slots=8):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_slots = num_slots
        
        # Attentionåˆ†æ”¯
        self.slot_attention = SlotAttention(feature_dim, num_slots)
        self.cross_attention = CrossAttention(feature_dim)
        
        # è‡ªé€‚åº”æƒé‡æ¨¡å—
        self.adaptive_weight = AdaptiveWeightModule(feature_dim)
        
    def forward(self, features, labels, unique_labels, centroids_list):
        """
        æ··åˆGMMå’ŒAttentionè®¡ç®—æ¡ä»¶ç†µ
        """
        # 1. GMMåˆ†æ”¯è®¡ç®—
        gmm_loss = self.compute_gmm_branch(features, labels, unique_labels, centroids_list)
        
        # 2. Attentionåˆ†æ”¯è®¡ç®—
        attention_loss = self.compute_attention_branch(features, labels, unique_labels)
        
        # 3. è‡ªé€‚åº”æƒé‡èåˆ
        gmm_weight, attention_weight = self.adaptive_weight(features)
        
        # 4. åŠ æƒèåˆ
        hybrid_loss = gmm_weight * gmm_loss + attention_weight * attention_loss
        
        return hybrid_loss
    
    def compute_gmm_branch(self, features, labels, unique_labels, centroids_list):
        """GMMåˆ†æ”¯ï¼šä¿æŒåŸå§‹GMMè®¡ç®—é€»è¾‘"""
        # è¿™é‡Œä¿æŒåŸå§‹çš„GMMè®¡ç®—é€»è¾‘
        # [åŸå§‹GMMä»£ç é€»è¾‘]
        pass
    
    def compute_attention_branch(self, features, labels, unique_labels):
        """Attentionåˆ†æ”¯ï¼šä½¿ç”¨Slot+Cross Attention"""
        total_conditional_entropy = 0.0
        total_samples = 0
        
        for label in unique_labels:
            class_mask = (labels == label.item())
            if class_mask.sum() <= 1:
                continue
                
            class_features = features[class_mask]
            
            # Slot Attention
            slot_representations = self.slot_attention(class_features)
            
            # Cross Attention
            enhanced_features = []
            for i in range(class_features.size(0)):
                enhanced_feat = self.cross_attention(
                    class_features[i:i+1], 
                    slot_representations[i:i+1]
                )
                enhanced_features.append(enhanced_feat)
            
            enhanced_features = torch.cat(enhanced_features, dim=0)
            
            # æ¡ä»¶ç†µè®¡ç®—
            if enhanced_features.shape[0] > 1:
                feature_variance = torch.var(enhanced_features, dim=0)
                conditional_entropy = torch.mean(torch.log(feature_variance + 1e-8))
            else:
                conditional_entropy = torch.tensor(0.0).to(features.device)
            
            total_conditional_entropy += conditional_entropy * class_mask.sum().float()
            total_samples += class_mask.sum().float()
        
        if total_samples > 0:
            return total_conditional_entropy / total_samples
        else:
            return torch.tensor(0.0).to(features.device)
```

**ä¿®æ”¹ä»£ç å—4: compute_class_meanså‡½æ•° (ç¬¬951-985è¡Œ)**
```python
def compute_class_means(self, features, labels, unique_labels, centroids_list):
    """
    ğŸš€ HYBRID GMM + ATTENTION: æ··åˆæ¶æ„è®¡ç®—æ¡ä»¶ç†µ
    """
    # å±•å¹³ç‰¹å¾
    if len(features.shape) == 4:
        batch_size, channels, height, width = features.shape
        features_flat = features.view(batch_size, -1)
        feature_dim = channels * height * width
    else:
        features_flat = features
        feature_dim = features.shape[1]
    
    # åˆå§‹åŒ–æ··åˆæ¨¡å—
    if not hasattr(self, 'hybrid_cem'):
        self.hybrid_cem = HybridGMMAttentionCEM(feature_dim=feature_dim).to(features.device)
    
    # ä½¿ç”¨æ··åˆæ¶æ„è®¡ç®—æ¡ä»¶ç†µ
    rob_loss, intra_class_mse = self.hybrid_cem(features_flat, labels, unique_labels, centroids_list)
    
    return rob_loss, intra_class_mse
```

#### **2-4. å…¶ä»–æ–‡ä»¶ä¿®æ”¹**
ä¸CEM-attç›¸åŒçš„matplotlibå’Œå¯¼å…¥ä¿®å¤ã€‚

---

## ğŸ¯ **CEM-enhanced é¡¹ç›®æ”¹åŠ¨è¯¦æƒ…**

### ğŸ“ **ä¿®æ”¹çš„æ–‡ä»¶**

#### **1. model_training.py** (ä¸»è¦æ”¹åŠ¨)

**æ–°å¢ä»£ç å—1: å¢å¼ºAttentionæ¨¡å— (ç¬¬42-194è¡Œ)**
```python
class EnhancedSlotAttention(nn.Module):
    """å¢å¼ºç‰ˆSlot Attention - å¤šå±‚çº§è” + æ®‹å·®è¿æ¥"""
    def __init__(self, feature_dim, num_slots=12, num_iterations=4, num_layers=2):
        super().__init__()
        self.num_slots = num_slots
        self.num_iterations = num_iterations
        self.num_layers = num_layers
        self.feature_dim = feature_dim
        
        # å¤šå±‚attention
        self.attention_layers = nn.ModuleList([
            nn.ModuleDict({
                'to_q': nn.Linear(feature_dim, feature_dim, bias=False),
                'to_k': nn.Linear(feature_dim, feature_dim, bias=False),
                'to_v': nn.Linear(feature_dim, feature_dim, bias=False),
                'norm': nn.LayerNorm(feature_dim),
                'mlp': nn.Sequential(
                    nn.Linear(feature_dim, feature_dim * 2),
                    nn.GELU(),
                    nn.Dropout(0.1),
                    nn.Linear(feature_dim * 2, feature_dim)
                )
            }) for _ in range(num_layers)
        ])
        
        # Slotåˆå§‹åŒ–
        self.slot_mu = nn.Parameter(torch.randn(1, 1, feature_dim))
        self.slot_log_sigma = nn.Parameter(torch.zeros(1, 1, feature_dim))
        
        # æ¸©åº¦é€€ç«å‚æ•°
        self.register_buffer('temperature', torch.tensor(1.0))
        
    def forward(self, features):
        batch_size = features.size(0)
        
        # åˆå§‹åŒ–slots
        mu = self.slot_mu.expand(batch_size, self.num_slots, -1)
        sigma = self.slot_log_sigma.exp().expand(batch_size, self.num_slots, -1)
        slots = mu + sigma * torch.randn_like(sigma)
        
        # å¤šå±‚çº§è”attention
        for layer_idx in range(self.num_layers):
            layer = self.attention_layers[layer_idx]
            
            for iter_idx in range(self.num_iterations):
                # æ¸©åº¦é€€ç«
                temp = self.temperature * (0.9 ** iter_idx)
                
                # Attentionè®¡ç®—
                q = layer['to_q'](slots)
                k = layer['to_k'](features.unsqueeze(1))
                v = layer['to_v'](features.unsqueeze(1))
                
                # æ³¨æ„åŠ›æƒé‡ + æ¸©åº¦é€€ç«
                attn = torch.softmax(torch.sum(q * k, dim=-1, keepdim=True) / (self.feature_dim ** 0.5 * temp), dim=1)
                
                # æ›´æ–°slots (æ®‹å·®è¿æ¥)
                updates = torch.sum(attn * v, dim=1, keepdim=True)
                slots_residual = layer['norm'](slots + layer['mlp'](updates.expand(-1, self.num_slots, -1)))
                slots = slots + slots_residual  # æ®‹å·®è¿æ¥
        
        # Slotç«äº‰æœºåˆ¶
        slot_importance = torch.norm(slots, dim=-1, keepdim=True)
        slots = slots * torch.softmax(slot_importance / 0.1, dim=1)
        
        return slots

class EnhancedCrossAttention(nn.Module):
    """å¢å¼ºç‰ˆCross Attention - æ›´å¤šå¤´ + LayerNorm + GELU"""
    def __init__(self, feature_dim, num_heads=8):
        super().__init__()
        self.num_heads = num_heads
        self.feature_dim = feature_dim
        self.head_dim = feature_dim // num_heads
        
        self.to_q = nn.Linear(feature_dim, feature_dim, bias=False)
        self.to_k = nn.Linear(feature_dim, feature_dim, bias=False)
        self.to_v = nn.Linear(feature_dim, feature_dim, bias=False)
        
        self.norm_q = nn.LayerNorm(feature_dim)
        self.norm_k = nn.LayerNorm(feature_dim)
        self.norm_v = nn.LayerNorm(feature_dim)
        
        self.to_out = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(feature_dim, feature_dim)
        )
        
    def forward(self, query_features, slot_features):
        batch_size = query_features.size(0)
        
        # å½’ä¸€åŒ– + ç”ŸæˆQ,K,V
        q = self.to_q(self.norm_q(query_features)).view(batch_size, 1, self.num_heads, self.head_dim)
        k = self.to_k(self.norm_k(slot_features)).view(batch_size, -1, self.num_heads, self.head_dim)
        v = self.to_v(self.norm_v(slot_features)).view(batch_size, -1, self.num_heads, self.head_dim)
        
        # Multi-head attention
        attn = torch.softmax(torch.sum(q * k, dim=-1) / (self.head_dim ** 0.5), dim=1)
        out = torch.sum(attn.unsqueeze(-1) * v, dim=1)
        out = out.view(batch_size, self.feature_dim)
        
        # æ®‹å·®è¿æ¥
        return query_features + self.to_out(out)
```

**æ–°å¢ä»£ç å—2: ä¸²è¡Œæ¶æ„æ ¸å¿ƒ (ç¬¬239-449è¡Œ)**
```python
class SerialAttentionGMMCEM(nn.Module):
    """ä¸²è¡ŒAttentionâ†’GMMæ¶æ„ + å¤šå°ºåº¦ç‰¹å¾èåˆ"""
    def __init__(self, feature_dim, num_classes=10, num_slots=12):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_classes = num_classes
        self.num_slots = num_slots
        
        # å¢å¼ºçš„Attentionæ¨¡å—
        self.slot_attention = EnhancedSlotAttention(feature_dim, num_slots, num_iterations=4)
        self.cross_attention = EnhancedCrossAttention(feature_dim, num_heads=8)
        
        # å¤šå°ºåº¦ç‰¹å¾å¤„ç†å™¨
        self.multi_scale_processor = nn.ModuleList([
            nn.Linear(feature_dim, feature_dim // 2),  # ä¸‹é‡‡æ ·
            nn.Linear(feature_dim, feature_dim * 2),   # ä¸Šé‡‡æ ·
            nn.Identity()  # åŸå§‹å°ºåº¦
        ])
        
        # è‡ªé€‚åº”ç‰¹å¾é—¨æ§
        self.feature_gate = nn.Sequential(
            nn.Linear(feature_dim * 4, feature_dim * 2),  # 4 = 0.5 + 2 + 1 + åŸå§‹
            nn.GELU(),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾é‡æ„æ¨¡å—
        self.feature_reconstruction = nn.Sequential(
            nn.Linear(feature_dim, feature_dim * 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.LayerNorm(feature_dim)
        )
        
        # å¢å¼ºçš„è‡ªé€‚åº”æƒé‡
        self.enhanced_adaptive_weight = EnhancedAdaptiveWeight(feature_dim)
        
    def forward(self, features, labels, unique_labels, centroids_list):
        """ä¸²è¡Œæ¶æ„ï¼šå¤šå°ºåº¦ç‰¹å¾ â†’ Attentionå¢å¼º â†’ GMMå¤„ç†"""
        
        # 1. å¤šå°ºåº¦ç‰¹å¾å¤„ç†
        multi_scale_features = []
        for processor in self.multi_scale_processor:
            if isinstance(processor, nn.Identity):
                multi_scale_features.append(features)
            else:
                processed = processor(features)
                # ç»´åº¦å¯¹é½
                if processed.size(1) != features.size(1):
                    if processed.size(1) < features.size(1):
                        processed = F.pad(processed, (0, features.size(1) - processed.size(1)))
                    else:
                        processed = processed[:, :features.size(1)]
                multi_scale_features.append(processed)
        
        # 2. è‡ªé€‚åº”ç‰¹å¾é—¨æ§
        concat_features = torch.cat(multi_scale_features + [features], dim=1)
        gate_weights = self.feature_gate(concat_features)
        gated_features = features * gate_weights
        
        # 3. ä¸²è¡ŒAttentionå¤„ç†
        attention_enhanced_features = self.process_attention_serial(gated_features, labels, unique_labels)
        
        # 4. ç‰¹å¾é‡æ„
        reconstructed_features = self.feature_reconstruction(attention_enhanced_features)
        
        # 5. å¢å¼ºçš„GMMå¤„ç†
        rob_loss = self.compute_enhanced_gmm_branch(reconstructed_features, labels, unique_labels, centroids_list)
        
        intra_class_mse = torch.tensor(0.0)
        return rob_loss, intra_class_mse
    
    def process_attention_serial(self, features, labels, unique_labels):
        """ä¸²è¡Œå¤„ç†ï¼šå…ˆSlot Attentionï¼Œå†Cross Attention"""
        enhanced_features = []
        
        for label in unique_labels:
            class_mask = (labels == label.item())
            if class_mask.sum() <= 1:
                enhanced_features.append(features[class_mask])
                continue
                
            class_features = features[class_mask]
            
            # æ­¥éª¤1: Slot Attention
            slot_representations = self.slot_attention(class_features)
            
            # æ­¥éª¤2: Cross Attention (é€ä¸ªå¤„ç†)
            class_enhanced = []
            for i in range(class_features.size(0)):
                enhanced_feat = self.cross_attention(
                    class_features[i:i+1], 
                    slot_representations[i:i+1]
                )
                class_enhanced.append(enhanced_feat)
            
            class_enhanced = torch.cat(class_enhanced, dim=0)
            
            # æ®‹å·®è¿æ¥ + LayerNorm
            class_enhanced = F.layer_norm(class_features + class_enhanced, class_enhanced.shape[1:])
            enhanced_features.append(class_enhanced)
        
        # é‡æ–°ç»„åˆ
        result = torch.zeros_like(features)
        start_idx = 0
        for label, enhanced_feat in zip(unique_labels, enhanced_features):
            class_mask = (labels == label.item())
            result[class_mask] = enhanced_feat
        
        return result
    
    def compute_enhanced_gmm_branch(self, features, labels, unique_labels, centroids_list):
        """å¢å¼ºçš„GMMåˆ†æ”¯ï¼šå¤„ç†attentionå¢å¼ºåçš„ç‰¹å¾"""
        total_reg_mutual_infor = 0.0
        total_weight = 0.0
        
        for i in unique_labels:
            i_item = i.item() if torch.is_tensor(i) else i
            centroids = centroids_list[i_item]
            class_mask = (labels == i_item)
            class_features = features[class_mask.squeeze(), :]
            
            if class_features.size(0) <= 1:
                continue
            
            # ğŸš€ å¢å¼ºè·ç¦»åº¦é‡ï¼šæ¬§æ°è·ç¦» + ä½™å¼¦ç›¸ä¼¼åº¦
            # æ¬§æ°è·ç¦»
            euclidean_distances = torch.cdist(class_features, centroids)
            # ä½™å¼¦ç›¸ä¼¼åº¦
            cos_sim = F.cosine_similarity(class_features.unsqueeze(1), centroids.unsqueeze(0), dim=2)
            cos_distances = 1 - cos_sim
            
            # åŠ¨æ€æƒé‡ç»„åˆ
            feature_var = torch.var(class_features, dim=0).mean()
            euclidean_weight = torch.sigmoid(feature_var)
            cosine_weight = 1 - euclidean_weight
            
            combined_distances = (euclidean_weight * euclidean_distances + 
                                cosine_weight * cos_distances)
            
            cluster_assignments = torch.argmin(combined_distances, dim=1).cpu().numpy()
            unique_cluster_assignments = np.unique(cluster_assignments)
            
            for j in unique_cluster_assignments:
                indice_cluster = cluster_assignments == j
                weight = sum(indice_cluster) / sum(class_mask.cpu().numpy())
                
                if sum(indice_cluster) <= 1:
                    continue
                
                cluster_features = class_features[indice_cluster]
                centroid = centroids[j]
                
                # ğŸš€ æ”¹è¿›çš„æ–¹å·®è®¡ç®—
                variances = torch.mean((cluster_features - centroid)**2, dim=0)
                
                # ğŸš€ è‡ªé€‚åº”æ­£åˆ™åŒ–
                adaptive_reg = 0.001 * (1 + feature_var)
                reg_variances = variances + adaptive_reg
                
                # ğŸš€ æ”¹è¿›çš„äº’ä¿¡æ¯è®¡ç®—
                epsilon = 1e-8
                mutual_infor = F.relu(torch.log(reg_variances + epsilon) - 
                                    torch.log(torch.tensor(adaptive_reg + epsilon)))
                
                reg_mutual_infor = mutual_infor.mean() * torch.tensor(weight)
                total_reg_mutual_infor += reg_mutual_infor
                total_weight += weight
        
        if total_weight > 0:
            return total_reg_mutual_infor / total_weight
        else:
            return torch.tensor(0.0).to(features.device)
```

**ä¿®æ”¹ä»£ç å—3: ç›´æ¥æŸå¤±èåˆ (ç¬¬1410-1461è¡Œ)**
```python
# ğŸš€ CEM-ENHANCED: ç›´æ¥æŸå¤±èåˆ - å…³é”®æ”¹è¿›ï¼
# æ¡ä»¶ç†µæŸå¤±ç›´æ¥å‚ä¸æ€»æŸå¤±ä¼˜åŒ–ï¼Œè€Œéæ¢¯åº¦ç´¯åŠ 
if not random_ini_centers and self.lambd > 0:
    total_loss = f_loss + self.lambd * rob_loss  # ğŸš€ ç›´æ¥èåˆæ¡ä»¶ç†µæŸå¤±
else:    
    total_loss = f_loss

# ç®€åŒ–è®­ç»ƒæµç¨‹ï¼šç›´æ¥åå‘ä¼ æ’­ï¼Œç§»é™¤å¤æ‚æ¢¯åº¦ç´¯åŠ 
total_loss.backward()
```

#### **2-5. å…¶ä»–æ–‡ä»¶ä¿®æ”¹**
ä¸å‰ä¸¤ä¸ªé¡¹ç›®ç›¸åŒçš„ä¿®å¤ã€‚

---

## ğŸ“Š **æ€»ç»“å¯¹æ¯”**

| é¡¹ç›® | æ ¸å¿ƒæ”¹åŠ¨ | ä»£ç è¡Œæ•° | ä¸»è¦æ–°å¢ç±» |
|------|---------|---------|-----------|
| **CEM-att** | ç”¨Attentionå®Œå…¨æ›¿æ¢GMM | ~200è¡Œ | `SlotAttention`, `CrossAttention`, `SlotCrossAttentionCEM` |
| **CEM-mix** | GMM+Attentionå¹¶è¡Œæ··åˆ | ~300è¡Œ | ä¸Šè¿°3ä¸ª + `AdaptiveWeightModule`, `HybridGMMAttentionCEM` |
| **CEM-enhanced** | ä¸²è¡Œæ¶æ„+å¤šå°ºåº¦ç‰¹å¾ | ~500è¡Œ | `EnhancedSlotAttention`, `EnhancedCrossAttention`, `SerialAttentionGMMCEM`, `EnhancedAdaptiveWeight` |

**å…±åŒä¿®æ”¹**:
- æ‰€æœ‰é¡¹ç›®éƒ½ä¿®å¤äº†matplotlibåç«¯é—®é¢˜
- æ‰€æœ‰é¡¹ç›®éƒ½ä¿®å¤äº†å¯¼å…¥é”™è¯¯
- æ‰€æœ‰é¡¹ç›®éƒ½è°ƒæ•´äº†å®éªŒå‚æ•°èŒƒå›´
- CEM-enhancedè¿˜åŠ å…¥äº†ç›´æ¥æŸå¤±èåˆæ”¹è¿›

è¿™å°±æ˜¯ä¸‰ä¸ªé¡¹ç›®ç›¸å¯¹äºCEM-mainçš„æ‰€æœ‰ä»£ç æ”¹åŠ¨è¯¦æƒ…ï¼
