# CIFAR-10 GMM vs Attention 特征分类方法比较实验报告

## 实验概述

本实验比较了两种特征分类方法在CIFAR-10数据集上的性能：
1. **GMM方法**: 基于高斯混合模型的特征分类
2. **Attention方法**: 基于Slot Attention和Cross Attention的特征分类

## 实验配置

- **数据集**: CIFAR-10 (50,000训练样本，10,000测试样本)
- **Backbone**: VGG11_bn (cutting_layer=4)
- **训练轮数**: 10 epochs
- **批次大小**: 128
- **设备**: NVIDIA GeForce RTX 3060 Laptop GPU
- **随机种子**: 123 (确保可重现性)

## 实验结果

### 性能指标对比

| 指标 | Attention方法 | GMM方法 | 差异 |
|------|---------------|---------|------|
| 训练时间 (秒) | 295.6 | 440.0 | -144.4 |
| 交叉熵损失 | 2.3239 | 2.3044 | -0.0195 |
| Top-1准确率 (%) | 10.14 | 9.26 | +0.88 |

### 详细分析

#### 1. 准确率表现
- **Attention方法**: 10.14% 准确率
- **GMM方法**: 9.26% 准确率
- **优势**: Attention方法比GMM方法高0.88%的准确率

#### 2. 损失函数表现
- **Attention方法**: 2.3239 交叉熵损失
- **GMM方法**: 2.3044 交叉熵损失
- **优势**: GMM方法有更低的交叉熵损失（差异0.0195）

#### 3. 训练效率
- **Attention方法**: 295.6秒训练时间
- **GMM方法**: 440.0秒训练时间
- **优势**: Attention方法训练速度更快（节省144.4秒，约33%）

## 方法特点分析

### GMM方法特点
- **优势**: 
  - 更低的交叉熵损失，表明模型对数据分布拟合更好
  - 基于概率模型，具有理论上的可解释性
- **劣势**: 
  - 训练时间较长
  - 准确率相对较低

### Attention方法特点
- **优势**: 
  - 更高的分类准确率
  - 训练效率更高
  - 能够学习更复杂的特征表示
- **劣势**: 
  - 交叉熵损失相对较高
  - 模型复杂度较高

## 结论

1. **准确率优先**: 如果需要更高的分类准确率，建议使用Attention方法
2. **效率优先**: 如果关注训练效率，Attention方法明显更优
3. **损失优化**: 如果追求更低的交叉熵损失，GMM方法表现更好

## 技术实现细节

### 特征提取
- 使用VGG11_bn作为backbone网络
- 在第4层进行特征切割，提取中间特征
- 特征维度: 128通道 × 8×8 空间维度

### Attention方法实现
- 使用Slot Attention进行特征聚类
- 8个attention heads，8个slots
- 0.1的dropout率防止过拟合

### GMM方法实现
- 每个类别使用3个高斯组件
- 使用PyTorch实现，支持GPU加速
- 包含PCA降维和逆变换

## 实验环境

- **Python版本**: 3.12
- **PyTorch版本**: 最新稳定版
- **CUDA版本**: 支持CUDA的GPU
- **内存使用**: 约6.4GB GPU内存

## 未来改进方向

1. **超参数调优**: 可以进一步优化两种方法的超参数
2. **更多数据集**: 在其他数据集上验证结果
3. **模型融合**: 考虑结合两种方法的优势
4. **更长时间训练**: 增加训练轮数观察长期表现

## 文件说明

- `run_full_compare_cifar10.py`: 主实验脚本
- `comparison_results_20250811_181846.json`: 详细实验结果
- `EXPERIMENT_SUMMARY.md`: 本实验报告

---

**实验完成时间**: 2025年8月11日 18:18:46  
**实验状态**: 成功完成 ✅
