# CEM-att: CEM with Attention Mechanism

è¿™æ˜¯åŸºäºåŸå§‹CEM-mainé¡¹ç›®ï¼Œå°†GMMåˆ†ç±»å™¨æ›¿æ¢ä¸ºAttentionåˆ†ç±»å™¨çš„å®ç°ã€‚

## ä¸»è¦æ”¹åŠ¨

### 1. æ–°å¢æ–‡ä»¶
- `attention_modules.py`: Slot Attentionå’ŒCross Attentionå®ç°
- `quick_test.py`: å¿«é€Ÿæµ‹è¯•è„šæœ¬
- `README_ATTENTION.md`: æœ¬è¯´æ˜æ–‡ä»¶

### 2. ä¿®æ”¹æ–‡ä»¶
- `model_training.py`: æ·»åŠ attentionåˆ†ç±»å™¨æ”¯æŒ
- `main_MIA.py`: æ·»åŠ attentionç›¸å…³å‚æ•°
- `model_architectures/vgg.py`: ä¿®å¤bottleneckç›¸å…³bug

## ä½¿ç”¨æ–¹æ³•

### ğŸš€ ä¸€é”®è¿è¡Œè„šæœ¬ï¼ˆæ¨èï¼‰

#### æ–¹æ³•1ï¼šä½¿ç”¨ä¿®æ”¹ç‰ˆrun_expè„šæœ¬
```bash
# ç¼–è¾‘ run_exp_with_attention_option.sh 
# å°†ç¬¬8è¡Œæ”¹ä¸º: USE_ATTENTION=true  (å¯ç”¨attention)
# æˆ–ä¿æŒ: USE_ATTENTION=false (ä½¿ç”¨åŸå§‹GMM)
bash run_exp_with_attention_option.sh
```

#### æ–¹æ³•2ï¼šä½¿ç”¨ä¸“é—¨çš„attentionè„šæœ¬
```bash
# ç¼–è¾‘ run_exp_attention.sh
# å°†ç¬¬20è¡Œæ”¹ä¸º: use_attention=true/false
bash run_exp_attention.sh
```

#### æ–¹æ³•3ï¼šä½¿ç”¨åŸå§‹run_exp.sh
```bash
# åŸå§‹è„šæœ¬ä¼šä½¿ç”¨GMMæ–¹æ³•ï¼ˆä¸æ”¯æŒattentionï¼‰
bash run_exp.sh
```

### åŸºæœ¬è®­ç»ƒå‘½ä»¤

#### ä½¿ç”¨Attentionåˆ†ç±»å™¨ï¼ˆæ–°åŠŸèƒ½ï¼‰:
```bash
python main_MIA.py \
    --filename cem_attention_exp \
    --arch vgg11_bn \
    --cutlayer 4 \
    --batch_size 128 \
    --num_epochs 100 \
    --learning_rate 0.01 \
    --lambd 1.0 \
    --dataset cifar10 \
    --use_attention_classifier \
    --num_slots 8 \
    --attention_heads 8 \
    --attention_dropout 0.1 \
    --log_entropy 1
```

#### ä½¿ç”¨åŸå§‹GMMåˆ†ç±»å™¨ï¼ˆåŸºçº¿ï¼‰:
```bash
python main_MIA.py \
    --filename cem_baseline_exp \
    --arch vgg11_bn \
    --cutlayer 4 \
    --batch_size 128 \
    --num_epochs 100 \
    --learning_rate 0.01 \
    --lambd 1.0 \
    --dataset cifar10 \
    --log_entropy 1
```

### æ–°å¢å‚æ•°è¯´æ˜

- `--use_attention_classifier`: å¯ç”¨attentionåˆ†ç±»å™¨ï¼ˆä¸åŠ æ­¤å‚æ•°åˆ™ä½¿ç”¨åŸå§‹GMMï¼‰
- `--num_slots`: Slot Attentionçš„slotæ•°é‡ï¼ˆé»˜è®¤8ï¼‰
- `--attention_heads`: æ³¨æ„åŠ›å¤´æ•°ï¼ˆé»˜è®¤8ï¼‰
- `--attention_dropout`: Attentionæ¨¡å—çš„dropoutç‡ï¼ˆé»˜è®¤0.1ï¼‰

## æŠ€æœ¯ç»†èŠ‚

### Attentionæœºåˆ¶æ›¿ä»£GMMçš„åŸç†

1. **åŸå§‹GMMæ–¹æ³•**:
   - ä¸ºæ¯ä¸ªç±»åˆ«è®­ç»ƒç‹¬ç«‹çš„é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆ3ä¸ªé«˜æ–¯ç»„ä»¶ï¼‰
   - ä½¿ç”¨å›ºå®šçš„é«˜æ–¯ä¸­å¿ƒè¿›è¡Œèšç±»
   - è®¡ç®—ç‰¹å¾åˆ°é«˜æ–¯ä¸­å¿ƒçš„è·ç¦»æ¥ä¼°è®¡æ¡ä»¶ç†µ

2. **Attentionæ–¹æ³•**:
   - ä½¿ç”¨Slot Attentionå­¦ä¹ åŠ¨æ€çš„slotè¡¨ç¤ºï¼ˆ8ä¸ªslotsï¼‰
   - Slotè¡¨ç¤ºä½œä¸ºåŠ¨æ€èšç±»ä¸­å¿ƒ
   - ä½¿ç”¨Cross Attentionå¢å¼ºç‰¹å¾è¡¨ç¤º
   - è®¡ç®—ç‰¹å¾åˆ°slotä¸­å¿ƒçš„è·ç¦»æ¥ä¼°è®¡æ¡ä»¶ç†µ

### å…³é”®å‡½æ•°

- `attention_classify_features()`: æ‰§è¡Œattentionå‰å‘ä¼ æ’­
- `compute_attention_conditional_entropy()`: è®¡ç®—åŸºäºattentionçš„æ¡ä»¶ç†µæŸå¤±

## è®¾å¤‡æ”¯æŒ

ä»£ç è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨å¯ç”¨è®¾å¤‡ï¼š
- Linux + NVIDIA GPU: è‡ªåŠ¨ä½¿ç”¨CUDA
- macOS: ä½¿ç”¨CPUæˆ–MPSï¼ˆå¦‚æœå¯ç”¨ï¼‰
- å…¶ä»–: ä½¿ç”¨CPU

## æ–‡ä»¶ç»“æ„

```
CEM-att/
â”œâ”€â”€ attention_modules.py          # Attentionæœºåˆ¶å®ç°
â”œâ”€â”€ model_training.py            # ä¸»è®­ç»ƒç±»ï¼ˆå·²ä¿®æ”¹ï¼‰
â”œâ”€â”€ main_MIA.py                  # è®­ç»ƒè„šæœ¬ï¼ˆå·²ä¿®æ”¹ï¼‰
â”œâ”€â”€ quick_test.py                # å¿«é€Ÿæµ‹è¯•
â”œâ”€â”€ README_ATTENTION.md          # æœ¬æ–‡ä»¶
â”œâ”€â”€ model_architectures/         # ç½‘ç»œæ¶æ„
â”œâ”€â”€ datasets_torch.py            # æ•°æ®åŠ è½½
â”œâ”€â”€ utils.py                     # å·¥å…·å‡½æ•°
â””â”€â”€ [å…¶ä»–åŸå§‹CEMæ–‡ä»¶]            # å®Œå…¨ä¿æŒä¸å˜
```

## éªŒè¯

åœ¨éƒ¨ç½²å‰å¯è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼š
```bash
python quick_test.py
```

## é¢„æœŸæ”¹è¿›

ç›¸æ¯”åŸå§‹GMMæ–¹æ³•ï¼ŒAttentionæœºåˆ¶é¢„æœŸæä¾›ï¼š
1. **åŠ¨æ€å­¦ä¹ **: slotè¡¨ç¤ºå¯è‡ªé€‚åº”è°ƒæ•´
2. **ç«¯åˆ°ç«¯ä¼˜åŒ–**: æ•´ä¸ªç³»ç»Ÿè”åˆè®­ç»ƒ
3. **æ›´å¼ºè¡¨è¾¾èƒ½åŠ›**: çªç ´é«˜æ–¯åˆ†å¸ƒå‡è®¾é™åˆ¶
4. **æ›´å¥½çš„æ¡ä»¶ç†µä¼°è®¡**: åŸºäºå­¦ä¹ çš„èšç±»ä¸­å¿ƒ

## å…¼å®¹æ€§

- å®Œå…¨å‘åå…¼å®¹åŸå§‹CEM-main
- ä¸ä½¿ç”¨`--use_attention_classifier`æ—¶è¡Œä¸ºä¸åŸå§‹ç‰ˆæœ¬å®Œå…¨ç›¸åŒ
- æ‰€æœ‰åŸå§‹å‚æ•°å’ŒåŠŸèƒ½ä¿æŒä¸å˜
