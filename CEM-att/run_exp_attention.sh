#!/bin/bash

# CEM-att å®éªŒè„šæœ¬ - æ”¯æŒAttentionæœºåˆ¶
# åŸºäºåŸå§‹run_exp.shï¼Œæ·»åŠ attentionå‚æ•°æ”¯æŒ

GPU_id=0
arch=vgg11_bn  # ä½¿ç”¨æ ‡å‡†vgg11_bnï¼ˆä¸ç”¨sgmç‰ˆæœ¬ï¼‰
batch_size=128
random_seed=125
cutlayer_list="4"
num_client=1

# åŸå§‹CEMå‚æ•°
regularization='None'  # ç®€åŒ–ï¼Œä¸“æ³¨äºattention vs GMMå¯¹æ¯”
var_threshold=0.1
learning_rate=0.01
local_lr=-1
num_epochs=50  # å‡å°‘epochæ•°è¿›è¡Œå¿«é€Ÿæµ‹è¯•
lambd_list="1 16"  # æµ‹è¯•ä¸åŒçš„æ¡ä»¶ç†µæƒé‡
log_entropy=1

# Attentionç›¸å…³å‚æ•°
use_attention=true  # è®¾ç½®ä¸ºtrueä½¿ç”¨attentionï¼Œfalseä½¿ç”¨GMM
num_slots=8
attention_heads=8
attention_dropout=0.1

dataset_list="cifar10"
scheme=V2_epoch
folder_name="saves/attention_vs_gmm_comparison"

echo "ğŸš€ å¼€å§‹CEM-attå®éªŒ"
echo "=================================="
echo "ä½¿ç”¨Attentionæœºåˆ¶: $use_attention"
echo "æ•°æ®é›†: $dataset_list"
echo "Lambdaå€¼: $lambd_list"
echo "Epochs: $num_epochs"
echo "=================================="

for dataset in $dataset_list; do
    for lambd in $lambd_list; do
        for cutlayer in $cutlayer_list; do
            
            if [ "$use_attention" = "true" ]; then
                # ä½¿ç”¨Attentionåˆ†ç±»å™¨
                method="attention"
                filename=attention_lambd_${lambd}_epoch_${num_epochs}_slots_${num_slots}_heads_${attention_heads}
                
                echo "ğŸ¯ è®­ç»ƒ CEM + Attention (lambd=$lambd)"
                CUDA_VISIBLE_DEVICES=${GPU_id} python main_MIA.py \
                    --arch=${arch} \
                    --cutlayer=$cutlayer \
                    --batch_size=${batch_size} \
                    --filename=$filename \
                    --num_client=$num_client \
                    --num_epochs=$num_epochs \
                    --dataset=$dataset \
                    --scheme=$scheme \
                    --regularization=${regularization} \
                    --log_entropy=${log_entropy} \
                    --random_seed=$random_seed \
                    --learning_rate=$learning_rate \
                    --lambd=$lambd \
                    --local_lr $local_lr \
                    --folder ${folder_name} \
                    --var_threshold ${var_threshold} \
                    --use_attention_classifier \
                    --num_slots ${num_slots} \
                    --attention_heads ${attention_heads} \
                    --attention_dropout ${attention_dropout}
            else
                # ä½¿ç”¨åŸå§‹GMMåˆ†ç±»å™¨ï¼ˆåŸºçº¿å¯¹æ¯”ï¼‰
                method="gmm"
                filename=baseline_gmm_lambd_${lambd}_epoch_${num_epochs}
                
                echo "ğŸ“Š è®­ç»ƒ CEM + GMM åŸºçº¿ (lambd=$lambd)"
                CUDA_VISIBLE_DEVICES=${GPU_id} python main_MIA.py \
                    --arch=${arch} \
                    --cutlayer=$cutlayer \
                    --batch_size=${batch_size} \
                    --filename=$filename \
                    --num_client=$num_client \
                    --num_epochs=$num_epochs \
                    --dataset=$dataset \
                    --scheme=$scheme \
                    --regularization=${regularization} \
                    --log_entropy=${log_entropy} \
                    --random_seed=$random_seed \
                    --learning_rate=$learning_rate \
                    --lambd=$lambd \
                    --local_lr $local_lr \
                    --folder ${folder_name} \
                    --var_threshold ${var_threshold}
                    # æ³¨æ„ï¼šæ²¡æœ‰--use_attention_classifierå‚æ•°
            fi
            
            echo "âœ… å®Œæˆ $method è®­ç»ƒ (lambd=$lambd)"
            echo "--------------------------------"
            
        done
    done
done

echo "ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼"
echo "ğŸ“ ç»“æœä¿å­˜åœ¨: $folder_name"
echo "ğŸ” æ£€æŸ¥logså¯¹æ¯”attention vs GMMæ€§èƒ½"
