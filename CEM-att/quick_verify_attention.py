#!/usr/bin/env python3

"""
å¿«é€ŸéªŒè¯attentionåˆ†ç±»å™¨æ˜¯å¦åœ¨å®é™…ä½¿ç”¨
"""

import torch
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import model_training

def verify_attention_usage():
    """éªŒè¯attentionåˆ†ç±»å™¨æ˜¯å¦çœŸçš„åœ¨ä½¿ç”¨"""
    print("ğŸ” éªŒè¯Attentionåˆ†ç±»å™¨ä½¿ç”¨æƒ…å†µ...")
    
    # åˆå§‹åŒ–MIA_train (ä½¿ç”¨ä¸å®é™…è„šæœ¬ç›¸åŒçš„å‚æ•°)
    mi = model_training.MIA_train(
        arch='vgg11_bn_sgm',
        cutting_layer=4,
        batch_size=32,  # å°æ‰¹é‡
        lambd=16.0,  # ä¸å®é™…è„šæœ¬ç›¸åŒ
        n_epochs=1,
        scheme='V2_epoch',
        regularization_option='Gaussian_kl',
        regularization_strength=0.025,
        AT_regularization_option='SCA_new',
        AT_regularization_strength=0.3,
        bottleneck_option='noRELU_C8S1',
        gan_AE_type='res_normN4C64',
        gan_loss_type='SSIM',
        ssim_threshold=0.5,
        num_client=1,
        random_seed=125,
        log_entropy=1,
        var_threshold=0.125,
        # Attentionå‚æ•° (ä¸å®é™…è„šæœ¬ç›¸åŒ)
        use_attention_classifier=True,
        num_slots=8,
        attention_heads=8,
        attention_dropout=0.1
    )
    
    print(f"âœ… åˆå§‹åŒ–å®Œæˆ")
    print(f"ğŸ“‹ å…³é”®å‚æ•°æ£€æŸ¥:")
    print(f"   - use_attention_classifier: {mi.use_attention_classifier}")
    print(f"   - lambd: {mi.lambd}")
    print(f"   - attention_classifier: {mi.attention_classifier is not None}")
    
    if mi.attention_classifier is not None:
        print(f"   - feature_dim: {mi.attention_classifier.feature_dim}")
        print(f"   - num_slots: {mi.attention_classifier.num_slots}")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    device = mi.device
    batch_size = 8
    z_private = torch.randn(batch_size, 8, 8, 8).to(device)  # bottleneckåçš„ç‰¹å¾
    label_private = torch.randint(0, 10, (batch_size,)).to(device)
    
    print(f"\nğŸ§ª æµ‹è¯•attentionåˆ†ç±»å™¨å‰å‘ä¼ æ’­...")
    print(f"   - è¾“å…¥ç‰¹å¾å½¢çŠ¶: {z_private.shape}")
    print(f"   - æ ‡ç­¾å½¢çŠ¶: {label_private.shape}")
    
    # æµ‹è¯•attentionåˆ†ç±»å™¨
    try:
        attention_logits, enhanced_features, slot_representations, attention_weights = mi.attention_classify_features(z_private, label_private)
        print(f"âœ… Attentionå‰å‘ä¼ æ’­æˆåŠŸ!")
        print(f"   - attention_logitså½¢çŠ¶: {attention_logits.shape}")
        print(f"   - é¢„æµ‹ç±»åˆ«: {torch.argmax(attention_logits, dim=1)}")
        print(f"   - çœŸå®æ ‡ç­¾: {label_private}")
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åˆç†
        if attention_logits.shape == (batch_size, 10):
            print(f"âœ… è¾“å‡ºç»´åº¦æ­£ç¡® (batch_size={batch_size}, num_classes=10)")
        else:
            print(f"âŒ è¾“å‡ºç»´åº¦é”™è¯¯: æœŸæœ›({batch_size}, 10), å®é™…{attention_logits.shape}")
            
        # æ£€æŸ¥softmaxåçš„æ¦‚ç‡
        probs = torch.softmax(attention_logits, dim=1)
        print(f"   - é¢„æµ‹æ¦‚ç‡èŒƒå›´: [{probs.min().item():.3f}, {probs.max().item():.3f}]")
        
    except Exception as e:
        print(f"âŒ Attentionå‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•è®­ç»ƒæ­¥éª¤é€»è¾‘
    print(f"\nğŸ”„ æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤é€»è¾‘...")
    
    # æ¨¡æ‹Ÿç¬¬1ä¸ªepoch (random_ini_centers=True)
    random_ini_centers = True
    print(f"ğŸ“ ç¬¬1ä¸ªepoch (random_ini_centers={random_ini_centers}):")
    if mi.use_attention_classifier and mi.lambd > 0:
        print(f"   âœ… ä¼šä½¿ç”¨attentionåˆ†ç±»å™¨ (å³ä½¿random_ini_centers=True)")
    else:
        print(f"   âŒ ä¸ä¼šä½¿ç”¨attentionåˆ†ç±»å™¨")
    
    # æ¨¡æ‹Ÿç¬¬2ä¸ªepoch (random_ini_centers=False)  
    random_ini_centers = False
    print(f"ğŸ“ ç¬¬2ä¸ªepochå¼€å§‹ (random_ini_centers={random_ini_centers}):")
    if mi.use_attention_classifier and mi.lambd > 0:
        print(f"   âœ… ä¼šä½¿ç”¨attentionåˆ†ç±»å™¨")
    else:
        print(f"   âŒ ä¸ä¼šä½¿ç”¨attentionåˆ†ç±»å™¨")
    
    print(f"\nğŸ¯ ç»“è®º:")
    if mi.use_attention_classifier and mi.lambd > 0 and mi.attention_classifier is not None:
        print(f"âœ… Attentionåˆ†ç±»å™¨é…ç½®æ­£ç¡®ï¼Œåº”è¯¥ä¼šè¢«ä½¿ç”¨")
        print(f"âœ… ä»ç¬¬1ä¸ªepochå¼€å§‹å°±ä¼šä½¿ç”¨attentionåˆ†ç±»å™¨è¿›è¡Œåˆ†ç±»")
        print(f"âœ… æ¡ä»¶ç†µè®¡ç®—ä»ç¬¬2ä¸ªepochå¼€å§‹ä½¿ç”¨attentionæ–¹æ³•")
        return True
    else:
        print(f"âŒ Attentionåˆ†ç±»å™¨é…ç½®æœ‰é—®é¢˜")
        return False

if __name__ == "__main__":
    print("ğŸ§ª éªŒè¯CEM-attä¸­Attentionåˆ†ç±»å™¨çš„å®é™…ä½¿ç”¨æƒ…å†µ")
    print("=" * 60)
    
    success = verify_attention_usage()
    
    if success:
        print(f"\nğŸ‰ éªŒè¯æˆåŠŸï¼")
        print(f"ğŸ“Š æ‚¨çš„å®éªŒè¾“å‡ºå‡†ç¡®ç‡ä½å¯èƒ½æ˜¯å› ä¸º:")
        print(f"   1. è®­ç»ƒè¿˜åœ¨æ—©æœŸé˜¶æ®µ (éœ€è¦æ›´å¤šepoch)")
        print(f"   2. Attentionåˆ†ç±»å™¨éœ€è¦æ—¶é—´å­¦ä¹ åˆé€‚çš„è¡¨ç¤º")
        print(f"   3. æ¡ä»¶ç†µæŸå¤±å’Œåˆ†ç±»æŸå¤±çš„å¹³è¡¡éœ€è¦è°ƒæ•´")
        print(f"   \nğŸ’¡ å»ºè®®: è®©å®éªŒç»§ç»­è¿è¡Œï¼Œå‡†ç¡®ç‡åº”è¯¥ä¼šé€æ¸æå‡")
    else:
        print(f"\nâŒ éªŒè¯å¤±è´¥ï¼Œéœ€è¦ä¿®å¤é…ç½®é—®é¢˜")
        sys.exit(1)
