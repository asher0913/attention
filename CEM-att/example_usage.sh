#!/bin/bash

# CEM-att ä½¿ç”¨ç¤ºä¾‹è„šæœ¬
# åœ¨LinuxæœåŠ¡å™¨ä¸Šè¿è¡Œæ­¤è„šæœ¬è¿›è¡Œè®­ç»ƒ

echo "ğŸš€ CEM-att è®­ç»ƒç¤ºä¾‹"
echo "=================================="

# 1. Attentionåˆ†ç±»å™¨è®­ç»ƒï¼ˆæ–°æ–¹æ³•ï¼‰
echo "1ï¸âƒ£ è®­ç»ƒ CEM + Attention åˆ†ç±»å™¨"
python main_MIA.py \
    --filename cem_attention_cifar10 \
    --arch vgg11_bn \
    --cutlayer 4 \
    --batch_size 128 \
    --num_epochs 50 \
    --learning_rate 0.01 \
    --lambd 1.0 \
    --dataset cifar10 \
    --use_attention_classifier \
    --num_slots 8 \
    --attention_heads 8 \
    --attention_dropout 0.1 \
    --log_entropy 1

echo "=================================="

# 2. åŸºçº¿GMMåˆ†ç±»å™¨è®­ç»ƒï¼ˆå¯¹æ¯”ï¼‰
echo "2ï¸âƒ£ è®­ç»ƒ CEM + GMM åˆ†ç±»å™¨ï¼ˆåŸºçº¿å¯¹æ¯”ï¼‰"
python main_MIA.py \
    --filename cem_baseline_cifar10 \
    --arch vgg11_bn \
    --cutlayer 4 \
    --batch_size 128 \
    --num_epochs 50 \
    --learning_rate 0.01 \
    --lambd 1.0 \
    --dataset cifar10 \
    --log_entropy 1

echo "ğŸ‰ è®­ç»ƒå®Œæˆï¼æ£€æŸ¥ saves/ æ–‡ä»¶å¤¹æŸ¥çœ‹ç»“æœ"
