#!/usr/bin/env python3

"""
éªŒè¯CEM-attå®éªŒä¸­attentionæœºåˆ¶æ˜¯å¦çœŸçš„åœ¨è¿è¡Œ
"""

import torch
import argparse
import os
import sys

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import model_training

def verify_attention_in_training():
    """éªŒè¯è®­ç»ƒæ—¶attentionæ˜¯å¦è¢«è°ƒç”¨"""
    print("ğŸ” éªŒè¯Attentionæœºåˆ¶æ˜¯å¦åœ¨CEMè®­ç»ƒä¸­è¢«è°ƒç”¨...")
    
    # ä½¿ç”¨ä¸å®é™…è„šæœ¬ç›¸åŒçš„å‚æ•°
    mi = model_training.MIA_train(
        arch='vgg11_bn_sgm',
        cutting_layer=4,
        batch_size=128,
        lambd=16.0,  # ä¸è„šæœ¬ç›¸åŒ
        n_epochs=1,
        scheme='V2_epoch',
        regularization_option='Gaussian_kl',
        regularization_strength=0.025,  # ä¸è„šæœ¬ç›¸åŒ
        AT_regularization_option='SCA_new',
        AT_regularization_strength=0.3,
        bottleneck_option='noRELU_C8S1',
        gan_AE_type='res_normN4C64',
        gan_loss_type='SSIM',
        ssim_threshold=0.5,
        num_client=1,
        random_seed=125,
        log_entropy=1,
        var_threshold=0.125,
        # Attentionå‚æ•°
        use_attention_classifier=True,
        num_slots=8,
        attention_heads=8,
        attention_dropout=0.1
    )
    
    print(f"ğŸ“‹ å…³é”®å‚æ•°éªŒè¯:")
    print(f"   âœ… use_attention_classifier: {mi.use_attention_classifier}")
    print(f"   âœ… lambd: {mi.lambd}")
    print(f"   âœ… attention_classifierå­˜åœ¨: {mi.attention_classifier is not None}")
    print(f"   âœ… compute_attention_conditional_entropyæ–¹æ³•å­˜åœ¨: {hasattr(mi, 'compute_attention_conditional_entropy')}")
    print(f"   âœ… attention_classify_featuresæ–¹æ³•å­˜åœ¨: {hasattr(mi, 'attention_classify_features')}")
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
    device = mi.device
    batch_size = 8
    x_private = torch.randn(batch_size, 3, 32, 32).to(device)
    label_private = torch.randint(0, 10, (batch_size,)).to(device)
    
    print(f"\nğŸ§ª æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤...")
    
    # æµ‹è¯•è®­ç»ƒæ­¥éª¤é€»è¾‘
    z_private = mi.f(x_private)
    unique_labels = torch.unique(label_private)
    
    print(f"   ğŸ“Š ç‰¹å¾æå–: {z_private.shape}")
    print(f"   ğŸ“Š å”¯ä¸€æ ‡ç­¾: {unique_labels}")
    
    # æ£€æŸ¥æ¡ä»¶ç†µè®¡ç®—è·¯å¾„
    if mi.use_attention_classifier and mi.lambd > 0:
        print(f"   ğŸ¯ æ¡ä»¶: use_attention_classifier={mi.use_attention_classifier}, lambd={mi.lambd}")
        print(f"   âœ… å°†ä½¿ç”¨Attentionè·¯å¾„è®¡ç®—æ¡ä»¶ç†µ")
        
        try:
            # è°ƒç”¨attentionåˆ†ç±»å™¨
            attention_logits, enhanced_features, slot_representations, attention_weights = mi.attention_classify_features(z_private, label_private)
            print(f"   âœ… Attentionåˆ†ç±»å™¨è°ƒç”¨æˆåŠŸ!")
            print(f"      - attention_logits: {attention_logits.shape}")
            print(f"      - slot_representations: {slot_representations.shape}")
            
            # è°ƒç”¨attentionæ¡ä»¶ç†µè®¡ç®—
            rob_loss, intra_class_mse = mi.compute_attention_conditional_entropy(z_private, label_private, unique_labels, slot_representations)
            print(f"   âœ… Attentionæ¡ä»¶ç†µè®¡ç®—æˆåŠŸ!")
            print(f"      - rob_loss: {rob_loss.item():.4f}")
            print(f"      - intra_class_mse: {intra_class_mse.item():.4f}")
            
            print(f"\nğŸ‰ ç¡®è®¤: Attentionæœºåˆ¶æ­£åœ¨è¢«ä½¿ç”¨!")
            print(f"   ğŸ’¡ ä¸»åˆ†ç±»å™¨å‡†ç¡®ç‡ç›¸ä¼¼æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºä¸»åˆ†ç±»è·¯å¾„æ²¡å˜")
            print(f"   ğŸ’¡ Attentionåªæ”¹è¿›æ¡ä»¶ç†µè®¡ç®—ï¼Œå¸®åŠ©è®­ç»ƒä¼˜åŒ–")
            
            return True
            
        except Exception as e:
            print(f"   âŒ Attentionè°ƒç”¨å¤±è´¥: {e}")
            return False
    else:
        print(f"   âŒ ä¸ä¼šä½¿ç”¨Attentionè·¯å¾„")
        print(f"      - use_attention_classifier: {mi.use_attention_classifier}")
        print(f"      - lambd: {mi.lambd}")
        return False

def check_training_difference():
    """æ£€æŸ¥è®­ç»ƒå·®å¼‚çš„é¢„æœŸ"""
    print(f"\nğŸ“ˆ å…³äºå‡†ç¡®ç‡ç›¸ä¼¼çš„è§£é‡Š:")
    print(f"   âœ… è¿™æ˜¯æ­£ç¡®çš„ç°è±¡ï¼åŸå› :")
    print(f"   1. ä¸»åˆ†ç±»å™¨è·¯å¾„å®Œå…¨ç›¸åŒ (VGG11 + f_tail + classifier)")
    print(f"   2. GMM/Attentionåªè®¡ç®—æ¡ä»¶ç†µæŸå¤±ï¼Œä¸ç›´æ¥åˆ†ç±»")
    print(f"   3. åˆæœŸå‡†ç¡®ç‡ä¸»è¦æ¥è‡ªä¸»åˆ†ç±»å™¨ï¼Œä¸æ˜¯GMM")
    print(f"   4. Attentionçš„æ”¹è¿›ä¼šåœ¨è®­ç»ƒåæœŸä½“ç°åœ¨:")
    print(f"      - æ›´å¥½çš„ç‰¹å¾è¡¨ç¤ºå­¦ä¹ ")
    print(f"      - æ›´ç¨³å®šçš„æ”¶æ•›")
    print(f"      - å¯èƒ½ç•¥é«˜çš„æœ€ç»ˆå‡†ç¡®ç‡")
    print(f"      - æ›´å¥½çš„æ”»å‡»é˜²å¾¡æ€§èƒ½")
    
    print(f"\nğŸ” å¦‚ä½•éªŒè¯Attentionåœ¨èµ·ä½œç”¨:")
    print(f"   1. æ£€æŸ¥æ¡ä»¶ç†µæŸå¤±(rob_loss)çš„æ•°å€¼æ˜¯å¦ä¸åŒ")
    print(f"   2. è§‚å¯Ÿè®­ç»ƒåæœŸæ˜¯å¦æœ‰æ€§èƒ½å·®å¼‚")
    print(f"   3. æ¯”è¾ƒæœ€ç»ˆçš„æ”»å‡»æµ‹è¯•ç»“æœ(MSE/SSIM/PSNR)")
    print(f"   4. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ä¸­çš„mutual_infoå€¼")

if __name__ == "__main__":
    print("ğŸ”¬ CEM-att Attentionæœºåˆ¶è¿è¡ŒéªŒè¯")
    print("=" * 60)
    
    success = verify_attention_in_training()
    check_training_difference()
    
    if success:
        print(f"\nâœ… éªŒè¯ç»“è®º:")
        print(f"   ğŸ¯ Attentionæœºåˆ¶ç¡®å®åœ¨è¿è¡Œ")
        print(f"   ğŸ¯ å‡†ç¡®ç‡ç›¸ä¼¼æ˜¯æ­£å¸¸ç°è±¡")
        print(f"   ğŸ¯ æ‚¨çš„å®éªŒå®Œå…¨æ­£ç¡®!")
    else:
        print(f"\nâŒ éœ€è¦æ£€æŸ¥é…ç½®é—®é¢˜")
        
    print(f"\nğŸ’¡ å»ºè®®:")
    print(f"   - è®©å®éªŒç»§ç»­è¿è¡Œåˆ°ç»“æŸ")
    print(f"   - é‡ç‚¹å…³æ³¨æ¡ä»¶ç†µæŸå¤±çš„æ•°å€¼")
    print(f"   - æ¯”è¾ƒæœ€ç»ˆæ”»å‡»æµ‹è¯•ç»“æœ")
