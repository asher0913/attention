# ğŸ¯ æ¶æ„é”™è¯¯å·²å½»åº•ä¿®å¤ï¼

## âŒ **ä¹‹å‰çš„é‡å¤§ç†è§£é”™è¯¯**

æˆ‘ä¹‹å‰å®Œå…¨è¯¯è§£äº†CEMç®—æ³•çš„å·¥ä½œåŸç†ï¼š

### **é”™è¯¯ç†è§£**ï¼š
- ä»¥ä¸ºè¦ç”¨attention **å®Œå…¨æ›¿ä»£æ•´ä¸ªåˆ†ç±»å™¨**
- å¯¼è‡´è·³è¿‡äº†åŸå§‹çš„ `VGG11 â†’ f_tail â†’ classifier` è·¯å¾„
- ç›´æ¥ç”¨ `attention_logits` ä½œä¸ºæœ€ç»ˆåˆ†ç±»è¾“å‡º
- ç»“æœï¼šå‡†ç¡®ç‡ä» 63% æš´è·Œåˆ° 8%

## âœ… **æ­£ç¡®ç†è§£å’Œä¿®å¤**

### **æ­£ç¡®çš„CEMæ¶æ„**ï¼š

#### **åŸå§‹CEM-main**ï¼š
```
ä¸»åˆ†ç±»è·¯å¾„: è¾“å…¥ â†’ VGG11ç‰¹å¾ â†’ f_tail â†’ classifier â†’ åˆ†ç±»è¾“å‡º (63%å‡†ç¡®ç‡)
                    â†“
æ¡ä»¶ç†µè®¡ç®—: GMMè®¡ç®—(centroids_list) â†’ rob_loss â†’ åŠ å…¥æ€»æŸå¤±
```

#### **æ­£ç¡®çš„CEM-att**ï¼š
```
ä¸»åˆ†ç±»è·¯å¾„: è¾“å…¥ â†’ VGG11ç‰¹å¾ â†’ f_tail â†’ classifier â†’ åˆ†ç±»è¾“å‡º (åº”è¯¥ä¹Ÿæ˜¯63%)
                    â†“
æ¡ä»¶ç†µè®¡ç®—: Attentionè®¡ç®—(slot_representations) â†’ rob_loss â†’ åŠ å…¥æ€»æŸå¤±
```

### **å…³é”®ä¿®å¤å†…å®¹**ï¼š

1. **ğŸ”§ åˆ†ç±»è·¯å¾„ä¿®å¤**ï¼š
   ```python
   # ä¿®å¤å‰ (é”™è¯¯)ï¼š
   if self.use_attention_classifier:
       output = attention_logits  # âŒ å®Œå…¨æ›¿ä»£äº†åˆ†ç±»å™¨!
   
   # ä¿®å¤å (æ­£ç¡®)ï¼š
   # ALWAYS use the original classification path
   output = self.f_tail(z_private_n)
   # ... ç»§ç»­åŸå§‹åˆ†ç±»è·¯å¾„
   ```

2. **ğŸ”§ æ¡ä»¶ç†µè®¡ç®—ä¿®å¤**ï¼š
   - **åŸæ¥**: `compute_class_means(z_private, labels, unique_labels, centroids_list)`
   - **ç°åœ¨**: `compute_attention_conditional_entropy(z_private, labels, unique_labels, slot_representations)`
   - **åŒºåˆ«**: åªæœ‰è®¡ç®—æ¡ä»¶ç†µçš„æ–¹æ³•ä¸åŒï¼Œä¸»åˆ†ç±»è·¯å¾„å®Œå…¨ç›¸åŒï¼

3. **ğŸ”§ è®­ç»ƒæµç¨‹ä¿®å¤**ï¼š
   ```python
   # æ¡ä»¶ç†µè®¡ç®— (åªåœ¨è¿™é‡Œä½¿ç”¨attention)
   if self.use_attention_classifier:
       attention_logits, enhanced_features, slot_representations, attention_weights = self.attention_classify_features(z_private, label_private)
       rob_loss, intra_class_mse = self.compute_attention_conditional_entropy(z_private, label_private, unique_labels, slot_representations)
   else:
       rob_loss, intra_class_mse = self.compute_class_means(z_private, label_private, unique_labels, centroids_list)
   
   # ä¸»åˆ†ç±»è·¯å¾„ (ä¿æŒä¸å˜!)
   output = self.f_tail(z_private_n)
   # ... åŸå§‹åˆ†ç±»å™¨è·¯å¾„
   
   # æ€»æŸå¤± (åˆ†ç±»æŸå¤± + æ¡ä»¶ç†µæŸå¤±)
   total_loss = f_loss + self.lambd * rob_loss
   ```

---

## ğŸ¯ **é¢„æœŸæ•ˆæœ**

ä¿®å¤åï¼Œæ‚¨åº”è¯¥ç«‹å³çœ‹åˆ°ï¼š

1. **âœ… ç¬¬1ä¸ªepochå‡†ç¡®ç‡å°±æ¥è¿‘63%** (ä¸åŸCEM-mainç›¸åŒ)
2. **âœ… è®­ç»ƒè¿‡ç¨‹ç¨³å®šï¼ŒæŸå¤±åˆç†ä¸‹é™**
3. **âœ… Attentionåªæ”¹è¿›æ¡ä»¶ç†µè®¡ç®—ï¼Œä¸å½±å“ä¸»åˆ†ç±»**
4. **âœ… å¯èƒ½è·å¾—æ¯”åŸç‰ˆæ›´å¥½çš„æœ€ç»ˆæ€§èƒ½**

---

## ğŸš€ **è¿è¡Œä¿®å¤åçš„å®éªŒ**

```bash
cd CEM-att/
bash run_working_attention_experiment.sh
```

**ç°åœ¨è¾“å‡ºåº”è¯¥ç±»ä¼¼**ï¼š
```
ğŸ¯ å¼€å§‹è¿è¡Œä¿®å¤åçš„ CEM + Attention å®éªŒ...
âœ… Attentionå‚æ•°: Slots=8, Heads=8, Dropout=0.1
ğŸš€ å¼€å§‹è®­ç»ƒ...

Epoch 0  Test (client-0): Loss 2.XXX  Prec@1 60.000+  # âœ… ä¸€å¼€å§‹å°±é«˜å‡†ç¡®ç‡!
Epoch 1  Test (client-0): Loss 2.XXX  Prec@1 65.000+  # âœ… æŒç»­æå‡
...
```

---

## ğŸ“‹ **æŠ€æœ¯æ€»ç»“**

### **æ ¸å¿ƒåŸåˆ™**ï¼š
1. **ä¿æŒä¸»åˆ†ç±»å™¨ä¸å˜** - è¿™æ˜¯CEMé«˜å‡†ç¡®ç‡çš„æ¥æº
2. **åªæ›¿æ¢æ¡ä»¶ç†µè®¡ç®—** - è¿™æ‰æ˜¯GMMçš„çœŸæ­£ä½œç”¨
3. **Attentionä½œä¸ºè¾…åŠ©** - å¸®åŠ©è®¡ç®—æ›´å¥½çš„æ¡ä»¶ç†µæŸå¤±

### **å®ç°ç»†èŠ‚**ï¼š
- `compute_attention_conditional_entropy()` **å®Œå…¨é•œåƒ** åŸå§‹ `compute_class_means()` çš„é€»è¾‘
- åªæ˜¯ç”¨ `slot_representations` ä½œä¸ºèšç±»ä¸­å¿ƒæ›¿ä»£ `centroids_list[i]`
- ä¿æŒç›¸åŒçš„è·ç¦»è®¡ç®—ã€èšç±»åˆ†é…ã€æ–¹å·®è®¡ç®—ã€æ¡ä»¶ç†µå…¬å¼

ç°åœ¨è¿™æ‰æ˜¯**çœŸæ­£æ­£ç¡®çš„"ä»…æ›¿æ¢GMMåˆ†ç±»ä¸ºattentionåˆ†ç±»"**ï¼

ğŸ‰ **ç»ˆäºå®ç°äº†æ‚¨è¦æ±‚çš„ç²¾ç¡®æ›¿æ¢ï¼**
