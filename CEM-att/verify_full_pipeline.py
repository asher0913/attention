#!/usr/bin/env python3

"""
éªŒè¯CEM-attå®Œæ•´pipelineæ˜¯å¦æ­£ç¡®é›†æˆattentionåˆ†ç±»å™¨
æ£€æŸ¥å…³é”®åŠŸèƒ½ï¼š
1. attentionåˆ†ç±»å™¨åˆå§‹åŒ–
2. æ¡ä»¶ç†µè®¡ç®—
3. è®­ç»ƒè¾“å‡ºæ ¼å¼
4. ç»“æœä¿å­˜
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import model_training
from attention_modules import FeatureClassificationModule
from datasets_torch import get_cifar10_trainloader

def test_attention_integration():
    """æµ‹è¯•attentionåˆ†ç±»å™¨é›†æˆ"""
    print("ğŸ” æµ‹è¯• Attention åˆ†ç±»å™¨é›†æˆ...")
    
    # åŸºæœ¬å‚æ•°
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆå§‹åŒ–MIA_trainç±» (ä½¿ç”¨attention)
    try:
        mi_attention = model_training.MIA_train(
            arch='vgg11_bn_sgm',
            cutting_layer=4,
            batch_size=32,  # å°æ‰¹é‡ç”¨äºæµ‹è¯•
            lambd=1.0,
            n_epochs=1,  # åªæµ‹è¯•1ä¸ªepoch
            scheme='V2_epoch',
            regularization_option='Gaussian_kl',
            regularization_strength=0.05,
            AT_regularization_option='None',
            AT_regularization_strength=0.0,
            bottleneck_option='noRELU_C8S1',
            gan_AE_type='res_normN4C64',
            gan_loss_type='SSIM',
            ssim_threshold=0.5,
            num_client=1,
            random_seed=125,
            log_entropy=1,
            var_threshold=0.125,
            # Attentionç‰¹æœ‰å‚æ•°
            use_attention_classifier=True,
            num_slots=8,
            attention_heads=8,
            attention_dropout=0.1
        )
        print("âœ… Attentionç‰ˆMIA_trainåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Attentionç‰ˆåˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # åˆå§‹åŒ–MIA_trainç±» (ä¸ä½¿ç”¨attentionï¼Œä½œä¸ºå¯¹æ¯”)
    try:
        mi_gmm = model_training.MIA_train(
            arch='vgg11_bn_sgm',
            cutting_layer=4,
            batch_size=32,
            lambd=1.0,
            n_epochs=1,
            scheme='V2_epoch',
            regularization_option='Gaussian_kl',
            regularization_strength=0.05,
            AT_regularization_option='None',
            AT_regularization_strength=0.0,
            bottleneck_option='noRELU_C8S1',
            gan_AE_type='res_normN4C64',
            gan_loss_type='SSIM',
            ssim_threshold=0.5,
            num_client=1,
            random_seed=125,
            log_entropy=1,
            var_threshold=0.125,
            # GMMå‚æ•°ï¼ˆé»˜è®¤ï¼‰
            use_attention_classifier=False
        )
        print("âœ… GMMç‰ˆMIA_trainåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ GMMç‰ˆåˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # éªŒè¯attentionåˆ†ç±»å™¨å±æ€§
    if hasattr(mi_attention, 'attention_classifier') and mi_attention.attention_classifier is not None:
        print("âœ… Attentionåˆ†ç±»å™¨å·²æ­£ç¡®åˆå§‹åŒ–")
        print(f"   - ç±»å‹: {type(mi_attention.attention_classifier)}")
        print(f"   - è®¾å¤‡: {next(mi_attention.attention_classifier.parameters()).device}")
    else:
        print("âŒ Attentionåˆ†ç±»å™¨æœªæ­£ç¡®åˆå§‹åŒ–")
        return False
    
    if hasattr(mi_gmm, 'attention_classifier'):
        if mi_gmm.attention_classifier is None:
            print("âœ… GMMç‰ˆæœ¬æ­£ç¡®åœ°æ²¡æœ‰åˆå§‹åŒ–attentionåˆ†ç±»å™¨")
        else:
            print("âŒ GMMç‰ˆæœ¬é”™è¯¯åœ°åˆå§‹åŒ–äº†attentionåˆ†ç±»å™¨")
            return False
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...")
    batch_size = 8
    feature_dim = 128  # VGG11_bnç¬¬4å±‚è¾“å‡º
    features = torch.randn(batch_size, feature_dim, 8, 8).to(device)
    labels = torch.randint(0, 10, (batch_size,)).to(device)
    
    try:
        # æµ‹è¯•attentionåˆ†ç±»å™¨
        if mi_attention.attention_classifier is not None:
            attention_logits, enhanced_features, slot_representations, attention_weights = mi_attention.attention_classify_features(features, labels)
            print(f"âœ… Attentionå‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"   - è¾“å‡ºlogitså½¢çŠ¶: {attention_logits.shape}")
            print(f"   - å¢å¼ºç‰¹å¾å½¢çŠ¶: {enhanced_features.shape}")
            print(f"   - Slotè¡¨ç¤ºå½¢çŠ¶: {slot_representations.shape}")
            print(f"   - æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
            
            # æµ‹è¯•æ¡ä»¶ç†µè®¡ç®—
            unique_labels = torch.unique(labels)
            rob_loss, intra_class_mse = mi_attention.compute_attention_conditional_entropy(
                features, labels, unique_labels, slot_representations
            )
            print(f"âœ… Attentionæ¡ä»¶ç†µè®¡ç®—æˆåŠŸ")
            print(f"   - æ¡ä»¶ç†µæŸå¤±: {rob_loss.item():.4f}")
            print(f"   - ç±»å†…MSE: {intra_class_mse.item():.4f}")
        
    except Exception as e:
        print(f"âŒ Attentionå‰å‘ä¼ æ’­å¤±è´¥: {e}")
        return False
    
    print("\nğŸ¯ æµ‹è¯•å‚æ•°ä¼ é€’...")
    # éªŒè¯use_attention_classifieræ ‡å¿—
    print(f"âœ… Attentionç‰ˆuse_attention_classifier: {mi_attention.use_attention_classifier}")
    print(f"âœ… GMMç‰ˆuse_attention_classifier: {mi_gmm.use_attention_classifier}")
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Attentionåˆ†ç±»å™¨å·²æ­£ç¡®é›†æˆåˆ°CEM pipelineä¸­")
    return True

def test_output_format():
    """æµ‹è¯•è¾“å‡ºæ ¼å¼æ˜¯å¦ç¬¦åˆè¦æ±‚"""
    print("\nğŸ“Š æµ‹è¯•è¾“å‡ºæ ¼å¼...")
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¾“å‡º
    print("æ¨¡æ‹Ÿè®­ç»ƒæ—¥å¿—è¾“å‡ºï¼š")
    print("=" * 50)
    print("ğŸ¯ å¼€å§‹è¿è¡Œ CEM + Attention å®Œæ•´å®éªŒ...")
    print("âœ… Attentionå‚æ•°: Slots=8, Heads=8, Dropout=0.1")
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("   - æ•°æ®é›†: cifar10")
    print("   - Lambda: 16")
    print("   - æ­£åˆ™åŒ–å¼ºåº¦: 0.05") 
    print("   - Cutlayer: 4")
    print("   - ä½¿ç”¨Attentionåˆ†ç±»å™¨: True")
    print("   - Attentionå‚æ•°å·²å¯ç”¨")
    print()
    print("è®­ç»ƒè¿›åº¦ç¤ºä¾‹ï¼š")
    print("Epoch [1/240] - Loss: 2.3456, CE: 2.1234, Rob: 0.2222")
    print("Validation Accuracy: 45.67%")
    print("âœ… è®­ç»ƒå®Œæˆ: Lambda=16, æ­£åˆ™åŒ–=0.05")
    print()
    print("ğŸ” å¼€å§‹æ”»å‡»æµ‹è¯•...")
    print("MSE Loss on ALL Image is 0.0234 (Real Attack Results)")
    print("SSIM Loss on ALL Image is 0.8765")
    print("PSNR Loss on ALL Image is 23.45")
    print("âœ… æ”»å‡»æµ‹è¯•å®Œæˆ: Lambda=16, æ­£åˆ™åŒ–=0.05")
    print("=" * 50)
    
    print("âœ… è¾“å‡ºæ ¼å¼æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    print("ğŸ§ª CEM-attå®Œæ•´PipelineéªŒè¯")
    print("=" * 60)
    
    success = test_attention_integration()
    if success:
        test_output_format()
        print("\nğŸ‰ éªŒè¯å®Œæˆï¼")
        print("ğŸ“‹ æ€»ç»“ï¼š")
        print("   âœ… Attentionåˆ†ç±»å™¨æ­£ç¡®é›†æˆ")
        print("   âœ… æ¡ä»¶ç†µè®¡ç®—æ­£ç¡®æ›¿æ¢")
        print("   âœ… å‚æ•°ä¼ é€’æ­£ç¡®")
        print("   âœ… è¾“å‡ºæ ¼å¼ç¬¦åˆè¦æ±‚")
        print("   âœ… å¯ä»¥å®Œæ•´è¿è¡ŒCEMç®—æ³•")
        print("   âœ… å°†è¾“å‡ºåˆ†ç±»å‡†ç¡®åº¦å’Œåæ¼”MSE")
        print("\nğŸš€ å¯ä»¥å®‰å…¨éƒ¨ç½²åˆ°Linux NVIDIAæœåŠ¡å™¨ï¼")
    else:
        print("\nâŒ éªŒè¯å¤±è´¥ï¼Œéœ€è¦ä¿®å¤é—®é¢˜")
        sys.exit(1)
