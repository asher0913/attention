# CEMç®—æ³•æ”¹è¿›ç­–ç•¥ä¸å®ç°æ–¹æ¡ˆ

## å½“å‰é—®é¢˜åˆ†æ

æ ¹æ®å®éªŒç»“æœï¼Œä¸‰ç§æ¶æ„ï¼ˆGMMã€Attentionã€æ··åˆï¼‰æ€§èƒ½å·®å¼‚ä¸æ˜¾è‘—ï¼š
- GMM: 85.35%
- Attention: 85.49% 
- æ··åˆ: 85.12%

**æ ¹æœ¬åŸå› ï¼š**
1. æ¡ä»¶ç†µæŸå¤±åªé€šè¿‡æ¢¯åº¦ç´¯åŠ å½±å“è®­ç»ƒï¼Œä¸ç›´æ¥å‚ä¸æ€»æŸå¤±
2. åˆ†ç±»æŸå¤±å ä¸»å¯¼åœ°ä½ï¼Œæ¡ä»¶ç†µçš„å½±å“è¢«ç¨€é‡Š
3. å½“å‰çš„attentionæœºåˆ¶å¯èƒ½è¿˜ä¸å¤Ÿå¼ºå¤§æ¥æ›¿ä»£GMM

## æ”¹è¿›ç­–ç•¥

### ğŸš€ ç­–ç•¥1ï¼šç›´æ¥æŸå¤±èåˆï¼ˆæ¨èï¼‰

**åŸç†ï¼š** å°†æ¡ä»¶ç†µæŸå¤±ç›´æ¥åŠ å…¥æ€»æŸå¤±å‡½æ•°ï¼Œè€Œä¸æ˜¯ä»…é€šè¿‡æ¢¯åº¦ç´¯åŠ 

**å®ç°æ–¹æ¡ˆï¼š**
```python
# å½“å‰åšæ³•ï¼ˆä»…æ¢¯åº¦ç´¯åŠ ï¼‰
total_loss = f_loss  # åªæœ‰åˆ†ç±»æŸå¤±
rob_loss.backward(retain_graph=True)
# é€šè¿‡æ¢¯åº¦ç´¯åŠ å½±å“å‚æ•°æ›´æ–°

# æ”¹è¿›åšæ³•ï¼ˆç›´æ¥æŸå¤±èåˆï¼‰
total_loss = f_loss + self.lambd * rob_loss  # ç›´æ¥åŠ å…¥æ¡ä»¶ç†µæŸå¤±
```

**ä¼˜åŠ¿ï¼š**
- æ¡ä»¶ç†µæŸå¤±ç›´æ¥å‚ä¸ä¼˜åŒ–è¿‡ç¨‹
- å½±å“æ›´æ˜æ˜¾ï¼Œæ›´å®¹æ˜“è§‚å¯Ÿåˆ°å·®å¼‚
- ç†è®ºä¸Šæ›´ç¬¦åˆæŸå¤±å‡½æ•°è®¾è®¡åŸåˆ™

### ğŸ”„ ç­–ç•¥2ï¼šä¸²è¡Œæ¶æ„è®¾è®¡

**æ–¹æ¡ˆ2.1ï¼šGMMâ†’Attentionä¸²è¡Œ**
```python
# å…ˆç”¨GMMå¤„ç†ç‰¹å¾
gmm_enhanced_features = gmm_process(features)
# å†ç”¨Attentionè¿›ä¸€æ­¥å¤„ç†
final_features = attention_process(gmm_enhanced_features)
```

**æ–¹æ¡ˆ2.2ï¼šAttentionâ†’GMMä¸²è¡Œ**
```python
# å…ˆç”¨Attentionæå–é«˜çº§ç‰¹å¾
attention_features = attention_process(features)
# å†ç”¨GMMè¿›è¡Œèšç±»å’Œæ¡ä»¶ç†µè®¡ç®—
final_loss = gmm_conditional_entropy(attention_features)
```

### ğŸ¯ ç­–ç•¥3ï¼šç‰¹å¾ç©ºé—´æ”¹è¿›

**æ–¹æ¡ˆ3.1ï¼šå¤šå°ºåº¦ç‰¹å¾èåˆ**
```python
# åœ¨ä¸åŒç½‘ç»œå±‚æå–ç‰¹å¾
early_features = f_layers[:2](x)  # ä½çº§ç‰¹å¾
mid_features = f_layers[2:4](x)   # ä¸­çº§ç‰¹å¾  
high_features = f_layers[4:](x)   # é«˜çº§ç‰¹å¾

# å¯¹ä¸åŒå°ºåº¦ç‰¹å¾åˆ†åˆ«è®¡ç®—æ¡ä»¶ç†µ
multi_scale_entropy = (
    attention_entropy(early_features) +
    attention_entropy(mid_features) + 
    attention_entropy(high_features)
)
```

**æ–¹æ¡ˆ3.2ï¼šç‰¹å¾è§£è€¦ä¸é‡æ„**
```python
# å°†ç‰¹å¾åˆ†è§£ä¸ºéšç§ç›¸å…³å’Œä»»åŠ¡ç›¸å…³éƒ¨åˆ†
task_features, privacy_features = feature_disentangle(features)
# åªå¯¹éšç§ç›¸å…³ç‰¹å¾è®¡ç®—æ¡ä»¶ç†µ
privacy_entropy = attention_conditional_entropy(privacy_features)
# é‡æ„å®Œæ•´ç‰¹å¾ç”¨äºåˆ†ç±»
reconstructed_features = feature_reconstruct(task_features, privacy_features)
```

### ğŸ§  ç­–ç•¥4ï¼šæ³¨æ„åŠ›æœºåˆ¶å¢å¼º

**æ–¹æ¡ˆ4.1ï¼šå±‚æ¬¡åŒ–æ³¨æ„åŠ›**
```python
# å¤šå±‚æ³¨æ„åŠ›çº§è”
attention_layer1 = SlotAttention(features, num_slots=8)
attention_layer2 = SlotAttention(attention_layer1, num_slots=4)
attention_layer3 = SlotAttention(attention_layer2, num_slots=2)
final_representation = CrossAttention(features, attention_layer3)
```

**æ–¹æ¡ˆ4.2ï¼šè‡ªé€‚åº”Slotæ•°é‡**
```python
# æ ¹æ®ç‰¹å¾å¤æ‚åº¦åŠ¨æ€è°ƒæ•´slotæ•°é‡
complexity = estimate_feature_complexity(features)
adaptive_slots = int(8 + complexity * 4)  # 8-16ä¸ªslots
slot_outputs = SlotAttention(features, num_slots=adaptive_slots)
```

**æ–¹æ¡ˆ4.3ï¼šæ³¨æ„åŠ›æ­£åˆ™åŒ–**
```python
# æ·»åŠ æ³¨æ„åŠ›ç¨€ç–æ€§çº¦æŸ
attention_weights = compute_attention_weights(features)
sparsity_loss = torch.norm(attention_weights, p=1)
total_loss = f_loss + lambd * rob_loss + beta * sparsity_loss
```

### ğŸ“Š ç­–ç•¥5ï¼šæŸå¤±å‡½æ•°æ”¹è¿›

**æ–¹æ¡ˆ5.1ï¼šåŠ¨æ€Î»è°ƒèŠ‚**
```python
# æ ¹æ®è®­ç»ƒé˜¶æ®µåŠ¨æ€è°ƒæ•´Î»
if epoch < warmup_epochs:
    current_lambda = self.lambd * (epoch / warmup_epochs)
elif accuracy > target_acc:
    current_lambda = self.lambd * 2  # å‡†ç¡®ç‡è¾¾æ ‡åå¢å¼ºéšç§ä¿æŠ¤
else:
    current_lambda = self.lambd
```

**æ–¹æ¡ˆ5.2ï¼šå¤šç›®æ ‡ä¼˜åŒ–**
```python
# å¼•å…¥Paretoä¼˜åŒ–
accuracy_loss = classification_loss
privacy_loss = conditional_entropy_loss
diversity_loss = feature_diversity_regularization

# å¤šç›®æ ‡æƒé‡è‡ªé€‚åº”
weights = pareto_weight_update(accuracy_loss, privacy_loss, diversity_loss)
total_loss = weights[0]*accuracy_loss + weights[1]*privacy_loss + weights[2]*diversity_loss
```

### ğŸ”¬ ç­–ç•¥6ï¼šçŸ¥è¯†è’¸é¦å¢å¼º

**æ–¹æ¡ˆ6.1ï¼šæ•™å¸ˆ-å­¦ç”Ÿæ¡†æ¶**
```python
# ä½¿ç”¨å¼ºå¤§çš„é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºæ•™å¸ˆ
teacher_features = pretrained_model(x)
student_features = current_model(x)

# çŸ¥è¯†è’¸é¦æŸå¤±
distillation_loss = KL_divergence(student_features, teacher_features)
attention_enhanced_features = attention_mechanism(student_features, teacher_features)
```

## å®ç°ä¼˜å…ˆçº§å»ºè®®

### ğŸ¥‡ ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šç›´æ¥æŸå¤±èåˆ
**ç†ç”±ï¼š** æœ€ç›´æ¥æœ‰æ•ˆï¼Œå®ç°ç®€å•ï¼Œå½±å“æ˜æ˜¾

**å…·ä½“æ­¥éª¤ï¼š**
1. ä¿®æ”¹`total_loss`è®¡ç®—æ–¹å¼
2. å¢å¤§Î»å€¼æµ‹è¯•ï¼ˆå¦‚Î»=32, 64ï¼‰
3. å¯¹æ¯”ä¸‰ç§æ¶æ„åœ¨æ–°æŸå¤±å‡½æ•°ä¸‹çš„æ€§èƒ½

### ğŸ¥ˆ ç¬¬äºŒä¼˜å…ˆçº§ï¼šä¸²è¡Œæ¶æ„
**ç†ç”±ï¼š** å……åˆ†åˆ©ç”¨ä¸¤ç§æ–¹æ³•çš„äº’è¡¥æ€§

**å…·ä½“æ­¥éª¤ï¼š**
1. å®ç°Attentionâ†’GMMä¸²è¡Œ
2. å®ç°GMMâ†’Attentionä¸²è¡Œ  
3. å¯¹æ¯”ä¸²è¡Œvså¹¶è¡Œçš„æ•ˆæœ

### ğŸ¥‰ ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šå¤šå°ºåº¦ç‰¹å¾
**ç†ç”±ï¼š** ä»æ ¹æœ¬ä¸Šæå‡ç‰¹å¾è¡¨è¾¾èƒ½åŠ›

**å…·ä½“æ­¥éª¤ï¼š**
1. åœ¨å¤šä¸ªç½‘ç»œå±‚æå–ç‰¹å¾
2. åˆ†åˆ«è®¡ç®—æ¡ä»¶ç†µå¹¶èåˆ
3. è¯„ä¼°å¤šå°ºåº¦æ–¹æ³•çš„æ•ˆæœ

## å¿«é€ŸéªŒè¯æ–¹æ¡ˆ

### æ–¹æ¡ˆAï¼šä¿®æ”¹æŸå¤±å‡½æ•°ï¼ˆ30åˆ†é’Ÿå®ç°ï¼‰
```python
# åœ¨CEM-mix/model_training.pyä¸­ä¿®æ”¹
# ç¬¬1079-1081è¡Œ
if not random_ini_centers and self.lambd>0:
    total_loss = f_loss + self.lambd * rob_loss  # ç›´æ¥åŠ å…¥æ¡ä»¶ç†µæŸå¤±
else:    
    total_loss = f_loss
```

### æ–¹æ¡ˆBï¼šå¢å¼ºÎ»å€¼æµ‹è¯•ï¼ˆ10åˆ†é’Ÿå®ç°ï¼‰
```bash
# æµ‹è¯•æ›´å¤§çš„Î»å€¼
python main_MIA.py --lambd 32 --regularization_strength 0.025
python main_MIA.py --lambd 64 --regularization_strength 0.025
```

### æ–¹æ¡ˆCï¼šä¸²è¡Œæ¶æ„ï¼ˆ2å°æ—¶å®ç°ï¼‰
```python
# åœ¨æ··åˆæ¶æ„ä¸­å®ç°ä¸²è¡Œå¤„ç†
def forward_serial(self, features, labels, unique_labels, centroids_list):
    # æ–¹æ¡ˆ1ï¼šAttention â†’ GMM
    attention_features = self.attention_process(features)
    gmm_loss = self.gmm_branch(attention_features, labels, unique_labels, centroids_list)
    
    # æ–¹æ¡ˆ2ï¼šGMM â†’ Attention  
    gmm_features = self.gmm_enhance_features(features, centroids_list)
    attention_loss = self.attention_branch(gmm_features, labels, unique_labels)
    
    return final_loss
```

## é¢„æœŸæ•ˆæœ

**ä¿å®ˆä¼°è®¡ï¼š**
- ç›´æ¥æŸå¤±èåˆï¼šå‡†ç¡®ç‡å·®å¼‚æ‰©å¤§åˆ°1-2%
- ä¸²è¡Œæ¶æ„ï¼šåœ¨å¤æ‚æ•°æ®ä¸Šæœ‰0.5-1%æå‡
- å¤šå°ºåº¦æ–¹æ³•ï¼šæ•´ä½“æ€§èƒ½æå‡1-3%

**ç†æƒ³æƒ…å†µï¼š**
- æ··åˆæ–¹æ³•æ¯”GMMé«˜2-5%
- éšç§ä¿æŠ¤æŒ‡æ ‡æ˜¾è‘—æ”¹å–„
- åœ¨ä¸åŒæ•°æ®é›†ä¸Šéƒ½æœ‰ä¸€è‡´çš„æå‡

## å®ç°å»ºè®®

å»ºè®®å…ˆä»**æ–¹æ¡ˆA**å¼€å§‹ï¼Œè¿™æ˜¯æœ€ç›´æ¥æœ‰æ•ˆçš„æ”¹è¿›ã€‚å¦‚æœæ•ˆæœæ˜æ˜¾ï¼Œå†è€ƒè™‘å®ç°ä¸²è¡Œæ¶æ„å’Œå¤šå°ºåº¦ç‰¹å¾æ–¹æ³•ã€‚

å…³é”®æ˜¯è¦è®©æ¡ä»¶ç†µæŸå¤±æœ‰è¶³å¤Ÿçš„å½±å“åŠ›ï¼Œå½“å‰çš„æ¢¯åº¦ç´¯åŠ æ–¹å¼å¯èƒ½ç¨€é‡Šäº†attentionæœºåˆ¶çš„ä¼˜åŠ¿ã€‚
