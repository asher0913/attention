diff -r --ex CEM-main/main_MIA.py CEM-att/main_MIA.py
6c6
< import model_training,model_training_paral,model_training_paral_pruning
---
> import model_training,model_training_paral_pruning
diff -r --ex CEM-main/main_test_MIA.py CEM-att/main_test_MIA.py
9c9
< import model_training,model_training_paral,model_training_paral_pruning
---
> import model_training,model_training_paral_pruning
diff -r --ex CEM-main/model_training.py CEM-att/model_training.py
33a34,218
> 
> # ============================================================================
> # Attention Mechanism for CEM (Replaces GMM in conditional entropy calculation)
> # ============================================================================
> 
> class SlotAttention(nn.Module):
>     """
>     Slot Attention module as requested by user:
>     1. 先对feature做一遍slot attention
>     2. 然后把这个slot attention作为一个KV输到一个cross attention里面
>     3. 然后Q就是原feature
>     """
>     def __init__(self, feature_dim, num_slots=8, num_iterations=3):
>         super().__init__()
>         self.num_slots = num_slots
>         self.num_iterations = num_iterations
>         self.feature_dim = feature_dim
>         
>         # Slot initialization parameters
>         self.slot_mu = nn.Parameter(torch.randn(1, 1, feature_dim))
>         self.slot_log_sigma = nn.Parameter(torch.zeros(1, 1, feature_dim))
>         
>         # Attention layers
>         self.to_q = nn.Linear(feature_dim, feature_dim, bias=False)
>         self.to_k = nn.Linear(feature_dim, feature_dim, bias=False)
>         self.to_v = nn.Linear(feature_dim, feature_dim, bias=False)
>         
>         # Update mechanism (simplified)
>         self.norm = nn.LayerNorm(feature_dim)
>         self.mlp = nn.Sequential(
>             nn.Linear(feature_dim, feature_dim),
>             nn.ReLU(),
>             nn.Linear(feature_dim, feature_dim)
>         )
> 
>     def forward(self, inputs):
>         batch_size = inputs.shape[0]
>         
>         # Initialize slots
>         mu = self.slot_mu.expand(batch_size, self.num_slots, -1)
>         sigma = self.slot_log_sigma.exp().expand(batch_size, self.num_slots, -1)
>         slots = mu + sigma * torch.randn_like(mu)
>         
>         # Iterative attention mechanism
>         for _ in range(self.num_iterations):
>             slots_prev = slots
>             
>             # Attention between slots and inputs
>             q = self.to_q(slots)  # [batch_size, num_slots, feature_dim]
>             k = self.to_k(inputs.unsqueeze(1))  # [batch_size, 1, feature_dim] 
>             v = self.to_v(inputs.unsqueeze(1))  # [batch_size, 1, feature_dim]
>             
>             # Compute attention weights
>             attn = torch.matmul(q, k.transpose(-2, -1)) / (self.feature_dim ** 0.5)
>             attn = F.softmax(attn, dim=-1)
>             
>             # Apply attention to values
>             updates = torch.matmul(attn, v)  # [batch_size, num_slots, feature_dim]
>             
>             # Update slots (simplified update instead of GRU)
>             slots = self.norm(slots_prev + updates)
>             slots = slots + self.mlp(slots)
>             
>         return slots
> 
> class CrossAttention(nn.Module):
>     """
>     Cross Attention module where:
>     - Q comes from original features
>     - K,V come from slot attention outputs
>     """
>     def __init__(self, feature_dim, num_heads=4):
>         super().__init__()
>         self.num_heads = num_heads
>         self.feature_dim = feature_dim
>         self.head_dim = feature_dim // num_heads
>         
>         self.to_q = nn.Linear(feature_dim, feature_dim, bias=False)
>         self.to_k = nn.Linear(feature_dim, feature_dim, bias=False)
>         self.to_v = nn.Linear(feature_dim, feature_dim, bias=False)
>         self.to_out = nn.Linear(feature_dim, feature_dim)
> 
>     def forward(self, query_features, slot_outputs):
>         batch_size = query_features.shape[0]
>         
>         # Query from original features
>         q = self.to_q(query_features).view(batch_size, 1, self.num_heads, self.head_dim).transpose(1, 2)
>         
>         # Key, Value from slot outputs  
>         k = self.to_k(slot_outputs).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
>         v = self.to_v(slot_outputs).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
>         
>         # Compute cross attention
>         attn = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
>         attn = F.softmax(attn, dim=-1)
>         
>         out = torch.matmul(attn, v)
>         out = out.transpose(1, 2).contiguous().view(batch_size, 1, self.feature_dim)
>         out = out.squeeze(1)  # Remove the sequence dimension
>         
>         return self.to_out(out)
> 
> class SlotCrossAttentionCEM(nn.Module):
>     """
>     Complete Slot + Cross Attention module for CEM conditional entropy calculation
>     Replaces GMM in compute_class_means function
>     """
>     def __init__(self, feature_dim=128, num_slots=8):
>         super().__init__()
>         self.num_slots = num_slots
>         self.feature_dim = feature_dim
>         
>         self.slot_attention = SlotAttention(feature_dim, num_slots)
>         self.cross_attention = CrossAttention(feature_dim)
> 
>     def forward(self, features, labels, unique_labels):
>         """
>         Replace GMM-based conditional entropy calculation with attention mechanism
>         
>         Args:
>             features: Input features [batch_size, feature_dim] (flattened)
>             labels: Labels for each sample
>             unique_labels: Unique label values
>             
>         Returns:
>             rob_loss: Conditional entropy loss (replaces GMM loss)
>             intra_class_mse: MSE within classes
>         """
>         total_loss = 0.0
>         total_mse = 0.0
>         
>         unique_labels = unique_labels.cpu().numpy()
>         labels = labels.cpu().numpy()
>         
>         for i in unique_labels:
>             # Get features for this class
>             class_mask = (labels == i)
>             class_features = features[class_mask.squeeze(), :]
>             
>             if class_features.shape[0] == 0:
>                 continue
>                 
>             # Apply Slot Attention to class features
>             slot_outputs = self.slot_attention(class_features)  # [num_samples, num_slots, feature_dim]
>             
>             # Apply Cross Attention: Q=original features, KV=slot outputs
>             enhanced_features = []
>             for j in range(class_features.shape[0]):
>                 enhanced_feat = self.cross_attention(
>                     class_features[j:j+1], 
>                     slot_outputs[j:j+1]
>                 )
>                 enhanced_features.append(enhanced_feat)
>             
>             if enhanced_features:
>                 enhanced_features = torch.cat(enhanced_features, dim=0)
>                 
>                 # Calculate class mean and variance (similar to original logic)
>                 class_mean = enhanced_features.mean(dim=0)
>                 mse = F.mse_loss(enhanced_features, class_mean.expand_as(enhanced_features))
>                 
>                 # Calculate conditional entropy-like loss
>                 if enhanced_features.shape[0] > 1:
>                     variances = torch.var(enhanced_features, dim=0, unbiased=False) + 0.001
>                     entropy_loss = torch.log(variances + 0.0001).mean()
>                 else:
>                     # Single sample case - use a small default entropy
>                     entropy_loss = torch.tensor(0.01, device=enhanced_features.device)
>                 
>                 total_loss += entropy_loss
>                 total_mse += mse
>         
>         # Average over classes
>         num_classes = len(unique_labels)
>         if num_classes > 0:
>             rob_loss = total_loss / num_classes
>             intra_class_mse = total_mse / num_classes
>         else:
>             rob_loss = torch.tensor(0.0, device=features.device)
>             intra_class_mse = torch.tensor(0.0, device=features.device)
>             
>         return rob_loss, intra_class_mse
> 
> # ============================================================================
> 
661,743c846,860
<     def compute_class_means(self, features, labels, unique_labels,centroids_list):
<         class_means = []
<         # intra_class_mse = 0.0
<         unique_labels=unique_labels.cpu().numpy()
<         labels=labels.cpu().numpy()
<         label_it = 0
<         for i in unique_labels:
<             centroids = centroids_list[i]
<             num_clusters = centroids.size(0)
<             class_mask = (labels == i)
<             class_features = features[class_mask.squeeze(), :] #能否用一个list把indice一次性储存起来
< 
<             class_mean = class_features.mean(dim=0)
<             
<             N, D = class_features.shape[0], class_features.shape[1] *class_features.shape[2] * class_features.shape[3]
<             class_features_flat = class_features.reshape(N, D)  # flatten
< 
<             distances = torch.cdist(class_features_flat, centroids).detach().cpu().numpy()  
<             
<             cluster_assignments = np.argmin(distances, axis=1)
<             unique_cluster_assignments = np.unique(cluster_assignments)
<             cluster_variances=[]
<             # average_variance=torch.tensor(0).cuda()
<             num=0
<             for j in unique_cluster_assignments:
<             #     # print([cluster_assignments == j], sum((cluster_assignments == j)), sum(class_mask))
<                 # cluster_assignments_np=np.array(cluster_assignments)
<                 indice_cluster=cluster_assignments == j
<                 # print(j)
<                 # print(indice_cluster,cluster_assignments_np)
<                 weight=sum(indice_cluster)/sum(class_mask)
<                 variances=torch.mean(((class_features_flat[indice_cluster] - centroids[j])**2),dim=0).cuda()
<                 # sorted_indices = torch.argsort(variances, descending=True)
<             #     # 取出前10个最大的值和它们的索引
<             #     # top_indices = sorted_indices[:10]
<             #     # top_values = variances[top_indices]
<                 # top_values, top_indices = torch.topk(variances, 5)
< 
<                 reg_variances = (variances+0.001)  #/ (self.regularization_strength**2+0.0001)
<                 # reg_variances = torch.exp(reg_variances)
<                 # reg_variances[top_indices] = reg_variances[top_indices]*0 4
<                 # reg_variances = torch.exp(reg_variances)
<                 # reg_variances=F.relu(reg_variances-0.2)
<                 mean_reg_variances=reg_variances.mean()*torch.tensor(weight)
< 
<                 mutual_infor= F.relu(torch.log(reg_variances+ 0.0001)-torch.log(torch.tensor(0.001)))
<                 reg_mutual_infor=mutual_infor.mean()*torch.tensor(weight)
< 
<                 # mean_mutual_infor = mutual_infor.mean()*torch.tensor(weight)
<                 # variances= torch.mean(top10_values,dim=0)
<                 # print(self.log_entropy==1,self.log_entropy)
<                 if num==0:
<                     if self.log_entropy==1:
<                         average_variance=reg_mutual_infor
<                         # print('log result')
<                     else:
<                         average_variance=mean_reg_variances
<                 else:
<                     if self.log_entropy==1:
<                         average_variance+=reg_mutual_infor
<                     else:
<                         average_variance+=mean_reg_variances
<                 num+=1
<                 # cluster_variances.append(variances) 
< 
<                 # print(((class_features_flat[cluster_assignments == j] - centroids[j])**2).shape)
<                 # covariance_matrix = torch.matmul(centered_data.T, centered_data) / sum((cluster_assignments == j))
<                 # # cluster_variances_mean = torch.mean(cluster_variances,0)  
<                 # covariance_matrix = covariance_matrix + self.regularization_strength* torch.eye(covariance_matrix.size(0)).cuda()
<                 # print(covariance_matrix)
<                 # det = torch.det(covariance_matrix+0.01)
<                 # print('det value is:', det)
<                 # mutal_info= torch.log(covariance_matrix)
<             # print(variances.shape,weight,variances)
<             # cluster_variances= torch.stack(cluster_variances)
<             # print(cluster_variances.shape)
< 
<             # cluster_variances = torch.stack([((class_features_flat[cluster_assignments == i] - centroids[i])**2).mean() for i in unique_cluster_assignments])
<             # average_variance = cluster_variances.mean()
<             # print(f"class_features_flat.requires_grad: {class_features_flat.requires_grad}")
<             # print(f"average_variance.requires_grad: {average_variance.requires_grad}")
<             if label_it==0:
<                 intra_class_mse=average_variance
---
>     def compute_class_means(self, features, labels, unique_labels, centroids_list):
>         """
>         ATTENTION REPLACEMENT: Use Slot + Cross Attention instead of GMM for conditional entropy
>         
>         This function now uses attention mechanism exactly as requested:
>         1. 先对feature做一遍slot attention
>         2. 然后把这个slot attention作为一个KV输到一个cross attention里面  
>         3. 然后Q就是原feature
>         """
>         # Initialize attention module on first use
>         if not hasattr(self, 'attention_cem'):
>             # Flatten features to get proper dimension
>             if len(features.shape) == 4:
>                 feature_dim = features.shape[1] * features.shape[2] * features.shape[3]
>                 features_flat = features.view(features.shape[0], -1)
745,756c862,878
<                 intra_class_mse+=average_variance
<             class_means.append(class_mean)
<             label_it+=1
<         intra_class_mse /= len(unique_labels)
<         class_means = torch.stack(class_means)  # Shape: [num_classes, 128, 8, 8]
<         class_mean_overall = class_means.mean(dim=0)  # 全局均值
<         inter_class_mse = F.mse_loss(class_means, class_mean_overall.expand_as(class_means))
<         loss = intra_class_mse# - 0.1*inter_class_mse
< 
<         # print(loss)
<         # print(centroids_list)
<         return loss,intra_class_mse
---
>                 feature_dim = features.shape[1]
>                 features_flat = features
>                 
>             self.attention_cem = SlotCrossAttentionCEM(feature_dim=feature_dim, num_slots=8)
>             # Move to same device as features
>             self.attention_cem = self.attention_cem.to(features.device)
>         
>         # Flatten features if needed
>         if len(features.shape) == 4:
>             features_flat = features.view(features.shape[0], -1)
>         else:
>             features_flat = features
>             
>         # Use attention mechanism instead of GMM
>         rob_loss, intra_class_mse = self.attention_cem(features_flat, labels, unique_labels)
>         
>         return rob_loss, intra_class_mse
diff -r --ex CEM-main/run_exp.sh CEM-att/run_exp.sh
27,28c27,28
< regularization_strength_list="0.01 0.025 0.05 0.1 0.15"
< lambd_list="0 8 16" #
---
> regularization_strength_list="0.025"
> lambd_list="16" #
